{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81fea442-224b-4c84-bb06-2e329b54f8fb",
   "metadata": {},
   "source": [
    "# LinkedIn Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a44344a-31bc-483a-a3ee-70eb4c0a3e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import random\n",
    "import regex as re\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "# Suppress the specific warning message\n",
    "warning_message = \"indexing past lexsort depth may impact performance.\"\n",
    "warnings.filterwarnings(\"ignore\", message=warning_message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3ac251-87da-40d0-88ad-b203ad5b8cea",
   "metadata": {},
   "source": [
    "## Iterate all States"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a42d88-95f3-480f-8895-985f40637072",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### URL Parsing\n",
    "Navigate to the second page of search results for \"data analyst\" positions in the United States, filter the results by the last month, and include all work types.\n",
    "\n",
    "![LinkedIn Job Listings](images/linkedin_job_list.png)<br><br>\n",
    "\n",
    "Here is the URL of this page:<br>\n",
    "<span style=\"background-color: #66ff66;\">https://&#8203;www&#8203;.linkedin.com/jobs/search/</span><span style=\"background-color: #ff6666;\">?</span><span style=\"background-color: yellow;\">currentJobId=3737393523</span><span style=\"background-color: #ff6666;\">&</span><span style=\"background-color: yellow;\">f_TPR=r2592000</span><span style=\"background-color: #ff6666;\">&</span><span style=\"background-color: yellow;\">f_WT=1%2C3%2C2</span><span style=\"background-color: #ff6666;\">&</span><span style=\"background-color: yellow;\">geoId=103977389</span><span style=\"background-color: #ff6666;\">&</span><span style=\"background-color: yellow;\">keywords=data%20analyst</span>\n",
    "<span style=\"background-color: #ff6666;\">&</span><span style=\"background-color: yellow;\">location=Washington%2C%20United%20States</span><span style=\"background-color: #ff6666;\">&</span><span style=\"background-color: yellow;\">origin=JOB_SEARCH_PAGE_JOB_FILTER</span><span style=\"background-color: #ff6666;\">&</span><span style=\"background-color: yellow;\">refresh=true</span><span style=\"background-color: #ff6666;\">&</span><span style=\"background-color: yellow;\">start=25</span>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2d900a54-0c5d-4145-a41f-7109c0f61086",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.linkedin.com/jobs-guest/jobs/api/seeMoreJobPostings/search?origin=JOB_SEARCH_PAGE_JOB_FILTER&refresh=true&position=1&pageNum=0&keywords=Data%20Analyst&location=Washington%20State&f_TPR=r2592000'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = SetFilter('Data Analyst', location='Washington State', day=30, job_num=0)\n",
    "test.create_url()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2aaa5dad-011c-455f-9dc6-a9f4b2524a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_states():\n",
    "    states = [\n",
    "        \"California\", \"Texas\", \"New York\", \"Washington\",\n",
    "        \"Alabama\", \"Arizona\", \"Arkansas\", \"Colorado\", \"Connecticut\", \"Delaware\", \"Florida\", \"Georgia\", \n",
    "        \"Idaho\", \"Illinois\", \"Indiana\", \"Iowa\", \"Kansas\", \"Kentucky\", \"Louisiana\", \"Maine\", \"Maryland\", \n",
    "        \"Massachusetts\", \"Michigan\", \"Minnesota\", \"Mississippi\", \"Missouri\", \"Montana\", \"Nebraska\", \n",
    "        \"Nevada\", \"New Hampshire\", \"New Jersey\", \"New Mexico\", \"North Carolina\", \"North Dakota\", \"Ohio\", \n",
    "        \"Oklahoma\", \"Oregon\", \"Pennsylvania\", \"South Carolina\", \"South Dakota\", \"Tennessee\", \"Utah\", \n",
    "        \"Vermont\", \"Virginia\", \"West Virginia\", \"Wisconsin\", \"Wyoming\", \"District of Columbia\"]\n",
    "    \n",
    "    return states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "048c3cbf-923e-42db-91ec-0c4c1d232c84",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SetFilter:\n",
    "    \n",
    "    def __init__(self, title=None, location=None, day=None, \n",
    "                 worktype=None, exp=None, job_num=None, early_applicant=False):\n",
    "        \n",
    "        assert (isinstance(day, int) and day > 0) or not day, 'day has to be an integer > 0 ()'\n",
    "        assert (worktype in ['On-site', 'Remote', 'Hybrid']) or not worktype, \"worktype is 'On-site', 'Remote', or 'Hybrid\"\n",
    "        assert (exp in ['Internship', 'Entry level', 'Associate', 'Mid-Senior level', 'Director']) or (not exp),\\\n",
    "                    \"exp is 'Internship', 'Entry level', 'Associate', 'Mid-Senior level', or 'Director'\"\n",
    "        assert (isinstance(job_num, int) and 0 < job_num < 1000) or not job_num, 'job_num has to be an integer bewteen 0 and 999'\n",
    "        assert isinstance(early_applicant, bool), 'early_applicant has to be boolean'\n",
    "        \n",
    "        self.title = title if title else None\n",
    "        self.location = location if location else 'United States'\n",
    "        self.day = day if day else None\n",
    "        self.worktype = worktype if worktype else None\n",
    "        self.exp = exp if exp else None\n",
    "        self.job_num = job_num if job_num else None\n",
    "        self.early_applicant = early_applicant\n",
    "\n",
    "    \n",
    "    def __call__(self):\n",
    "        return self.__dict__\n",
    " \n",
    "    \n",
    "    def update(self, **kwargs):\n",
    "        if 'title' in kwargs:\n",
    "            assert kwargs['title'] is None or isinstance(kwargs['title'], str), 'title must be a string or None'\n",
    "        if 'day' in kwargs:\n",
    "            assert (isinstance(kwargs['day'], int) and kwargs['day'] > 0) or (not kwargs['day']), 'day has to be an integer > 0'\n",
    "        if 'worktype' in kwargs:\n",
    "            assert (kwargs['worktype'] in ['On-site', 'Remote', 'Hybrid']) or (not kwargs['worktype']),\\\n",
    "                    \"worktype is 'On-site', 'Remote', or 'Hybrid'\"\n",
    "        if 'exp' in kwargs:\n",
    "            assert (kwargs['exp'] in ['Internship', 'Entry level', 'Associate', 'Mid-Senior level', 'Director']) \\\n",
    "                    or (not exp), \"exp is 'Internship', 'Entry level', 'Associate', 'Mid-Senior level', or 'Director'\"\n",
    "        if 'job_num' in kwargs:\n",
    "            assert (isinstance(kwargs['job_num'], int) and 0 <= kwargs['job_num'] < 1000) or not kwargs['job_num'],\\\n",
    "                    'job_num has to be an integer between 0 and 999'\n",
    "        if 'early_applicant' in kwargs:\n",
    "            assert isinstance(kwargs['early_applicant'], bool), 'early_applicant has to be boolean'\n",
    "        \n",
    "        for key, value in kwargs.items():\n",
    "            if hasattr(self, key):                \n",
    "                setattr(self, key, value)\n",
    "            else:\n",
    "                raise AttributeError(f\"'{type(self).__name__}' object has no attribute '{key}'\")\n",
    "    \n",
    "    \n",
    "    def get_basic_url(self):\n",
    "        ''' Return basic url for every url'''\n",
    "        return 'origin=JOB_SEARCH_PAGE_JOB_FILTER&refresh=true&position=1&pageNum=0'\n",
    "    \n",
    "    \n",
    "    def get_basic_webpage_url(self):\n",
    "        ''' Return basic url for usual search page'''\n",
    "        return 'https://www.linkedin.com/jobs/search?' + self.get_basic_url()      \n",
    "    \n",
    "    \n",
    "    def get_basic_api_url(self):\n",
    "        ''' Return basic url for backend API endpoint for loading more job postings'''\n",
    "        return 'https://www.linkedin.com/jobs-guest/jobs/api/seeMoreJobPostings/search?' + self.get_basic_url()\n",
    "    \n",
    "    \n",
    "    def get_title_url(self):\n",
    "        ''' Return title url'''  \n",
    "        title_header = '&keywords='\n",
    "        return f\"{title_header}{self.title.replace(' ', '%20')}\" if self.title else ''\n",
    "    \n",
    "    \n",
    "    def get_location_url(self):\n",
    "        ''' Return district url'''\n",
    "        location_header = '&location='\n",
    "        return f\"{location_header}{self.location.replace(' ', '%20')}\"\n",
    "    \n",
    "    \n",
    "    def get_day_url(self):\n",
    "        ''' Return day url'''\n",
    "        seconds_a_day = 86400\n",
    "        day_header = '&f_TPR=r'\n",
    "        return f'{day_header}{self.day*seconds_a_day}' if self.day else ''\n",
    "    \n",
    "    \n",
    "    def get_all_worktype_options_url(self):\n",
    "        ''' Return a dictionary of urls for all worktype options'''\n",
    "        return {'On-site': '&f_WT=1',\n",
    "                'Remote': '&f_WT=2',\n",
    "                'Hybrid': '&f_WT=3'}\n",
    "    \n",
    "    \n",
    "    def get_worktype_url(self):\n",
    "        ''' Return worktype url'''\n",
    "        options_url = self.get_all_worktype_options_url()        \n",
    "        return options_url[self.worktype] if self.worktype else ''\n",
    "        \n",
    "    \n",
    "    def get_all_exp_options_url(self):\n",
    "        ''' Return a dictionary of urls for all experience level options'''\n",
    "        return {'Internship': '&f_E=1',\n",
    "                'Entry level': '&f_E=2',\n",
    "                'Associate': '&f_E=3',\n",
    "                'Mid-Senior level': '&f_E=4',\n",
    "                'Director': '&f_E=5'}\n",
    "    \n",
    "    \n",
    "    def get_exp_url(self):\n",
    "        ''' Return experience level url'''\n",
    "        options_url = self.get_all_exp_options_url()\n",
    "        return options_url[self.exp] if self.exp else ''\n",
    "         \n",
    "        \n",
    "    def get_job_num_url(self):\n",
    "        ''' Return api page url'''\n",
    "        job_num_header = '&start='\n",
    "        return f'{job_num_header}{self.job_num}' if self.job_num else ''\n",
    "    \n",
    "    \n",
    "    def get_early_applicant_url(self):\n",
    "        return '&f_EA=true' if self.early_applicant else ''\n",
    "        \n",
    "    def create_url(self):    \n",
    "        ''' Return API page url if job_num is set else get usual webpage url'''\n",
    "        basic_url = self.get_basic_api_url() if self.job_num == None else self.get_basic_webpage_url()\n",
    "        return basic_url + self.get_title_url() + self.get_location_url() + self.get_day_url() + \\\n",
    "               self.get_worktype_url() + self.get_exp_url() + self.get_job_num_url() + self.get_early_applicant_url()\n",
    "    \n",
    "    \n",
    "    def get_filter_box_elements(self):\n",
    "            ''' Return the dictionary of filter dropdown boxes' elements on webpage'''\n",
    "            job_num = self.job_num\n",
    "            self.job_num = None\n",
    "            url = self.create_url()\n",
    "            self.job_num = job_num\n",
    "\n",
    "            # Try to get filter dropdown box elements\n",
    "            for try_ in range(20):\n",
    "                response = requests.get(url)\n",
    "                html = BeautifulSoup(response.content, 'html.parser')  \n",
    "                filter_boxes = {'day_element': [self.day, html.find(attrs={'aria-label': 'Date posted filter options'})],\n",
    "                    'worktype_element': [self.worktype, html.find(attrs={'aria-label': 'Remote filter options'})],\n",
    "                    'exp_element': [self.exp, html.find(attrs={'aria-label': 'Experience level filter options'})]}       \n",
    "                if None not in filter_boxes['day_element']:\n",
    "                    break\n",
    "                if try_ == 19:\n",
    "                    raise Exception('!!!!!!!!!!Filter Element Not Found!!!!!!!!!!')\n",
    "                time.sleep(random.randint(10,20))\n",
    "            return filter_boxes\n",
    "\n",
    "    \n",
    "    def get_job_counts(self):\n",
    "        # Get filter dropdown box elements\n",
    "        filter_boxes = self.get_filter_box_elements()\n",
    "        # Get job counts in filter elements\n",
    "        job_counts = {}   \n",
    "        check_symbol = '\\u2713'\n",
    "        # Iterate all filter dropdown box elements\n",
    "        for filter_name, filter_element in filter_boxes.items():\n",
    "            job_counts[filter_name] = []            \n",
    "            # Find the 'label' elements which include job counts information\n",
    "            # !!! Website may change aria-label names, go to filter_boxes in get_filter_box_elements()\n",
    "            job_count_elements = filter_element[1].find_all('label') \n",
    "            # Iterate all options in the dropdown box element\n",
    "            for option in job_count_elements:\n",
    "                string = '  ' + option.text.strip()\n",
    "                # Add a check symbol for current selected filter option\n",
    "                is_filter_selected = option.find_previous_sibling().has_attr('checked')\n",
    "                if is_filter_selected: \n",
    "                    string = check_symbol + string[1:]\n",
    "                    # If the day is not in filter options, webpage will still show 'Any Time'\n",
    "                    if filter_name == 'self.day' and self.day not in [1, 7, 30, None]:\n",
    "                        string = string.replace('Any Time', f'Past {self.day} Days') \n",
    "                job_counts[filter_name].append(string) \n",
    "        return job_counts\n",
    "    \n",
    "    \n",
    "    def show_job_counts(self):\n",
    "        job_counts = self.get_job_counts()   \n",
    "        # Find the maximum length among the lists\n",
    "        max_length = max(map(len, job_counts.values()))\n",
    "        # Pad the shorter lists with zero-space to make them equal in length\n",
    "        job_counts_df = {key: values + [' '] * (max_length - len(values)) for key, values in job_counts.items()}\n",
    "        # Calculate space needed between strings\n",
    "        job_counts_df = pd.DataFrame(job_counts_df)    \n",
    "        txt_len_df = job_counts_df.apply(lambda col: col.str.len())\n",
    "        pad = 10\n",
    "        space_df = txt_len_df.apply(lambda col: max(col)+pad-col)\n",
    "        \n",
    "        start = 20\n",
    "        longest_row = txt_len_df.iloc[:, -1].argmax()\n",
    "        longest_len = txt_len_df.iloc[longest_row, :].sum() + space_df.iloc[longest_row, :-1].sum()\n",
    "        print('\\n', ' '*(start-10), '-'*(longest_len+start))\n",
    "        header = f'*****  {self.location} ({self.title})  *****'\n",
    "        pre = len(header) // 2\n",
    "        mid = start + txt_len_df.iloc[0,0] + space_df.iloc[0,0] + txt_len_df.iloc[0,1] // 2 + 1\n",
    "        print(' '*(mid-pre), header, '\\n')\n",
    "        for idx, row in job_counts_df.iterrows():\n",
    "            print(' '*20, end='')\n",
    "            formatted_text = ''.join(f\"{string}{' '*space}\" for string, space in zip(row, space_df.iloc[idx]))\n",
    "            print(formatted_text)\n",
    "        print(' '*(start-10), '-'*(longest_len+start))\n",
    "        return job_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "463f4cbd-3559-4e7c-8a55-e4fb4e775a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decide_scrape_scope(job_counts):    \n",
    "    # [[day, worktype], [], [],...]\n",
    "    filter_ = []\n",
    "    day_jobs = []\n",
    "    worktypes = ['On-site', 'Remote', 'Hybrid']\n",
    "    for element in job_counts['day_element'][1:]:\n",
    "        pattern = r'([\\w\\s]+)\\(([0-9,]+)\\)'\n",
    "        day_jobs.append(int(re.search(pattern, element)[2].replace(',', '')))        \n",
    "\n",
    "\n",
    "    if day_jobs[0] <= 1200:\n",
    "        for worktype in worktypes:  \n",
    "                filter_.append([worktype, 30])\n",
    "    elif 1200 < day_jobs[0] <= 2000:\n",
    "        for worktype in worktypes:\n",
    "            for day in [7, 30]:\n",
    "                filter_.append([worktype, day])\n",
    "    elif 2000 < day_jobs[0] <= 4000:\n",
    "        if day_jobs[1] <= 2000:\n",
    "            for worktype in worktypes:\n",
    "                for day in [3, 7, 14, 21, 30]:\n",
    "                    filter_.append([worktype, day])\n",
    "        else:\n",
    "            for worktype in worktypes:\n",
    "                for day in [3, 5, 7, 14, 30]:\n",
    "                    filter_.append([worktype, day])\n",
    "    elif 4000 < day_jobs[0] <= 6000:\n",
    "        if day_jobs[1] <= 2000:\n",
    "           for worktype in worktypes: \n",
    "                for day in [3, 7, 10, 14, 21, 30]:\n",
    "                    filter_.append([worktype, day])\n",
    "        else:\n",
    "            for worktype in worktypes:\n",
    "                for day in [3, 5, 7, 14, 21, 30]:\n",
    "                    filter_.append([worktype, day])\n",
    "    elif 6000 < day_jobs[0] <= 8000:\n",
    "        if day_jobs[1] <= 2000:\n",
    "            for worktype in worktypes:\n",
    "                for day in [3, 7, 10, 15, 20, 25, 30]:\n",
    "                    filter_.append([worktype, day])\n",
    "        elif 1500 < day_jobs[1] <= 4000:\n",
    "           for worktype in worktypes: \n",
    "                for day in [1, 3, 7, 10, 14, 21, 30]:\n",
    "                    filter_.append([worktype, day])  \n",
    "        else:\n",
    "           for worktype in worktypes: \n",
    "                for day in [1, 3, 5, 7, 14, 21, 30]:\n",
    "                    filter_.append([worktype, day])\n",
    "    elif 8000 < day_jobs[0] <= 12000:\n",
    "        if day_jobs[1] <= 2000:\n",
    "           for worktype in worktypes: \n",
    "                for day in [3, 7, 10, 14, 18, 22, 26, 30]:\n",
    "                    filter_.append([worktype, day])\n",
    "        elif 2000 < day_jobs[1] <= 4000:\n",
    "            for worktype in worktypes:\n",
    "                for day in [1, 3, 7, 10, 15, 20, 25, 30]:\n",
    "                    filter_.append([worktype, day])  \n",
    "        elif 4000 < day_jobs[1] <= 8000:\n",
    "           for worktype in worktypes: \n",
    "                for day in [1, 2, 3, 5, 7, 14, 21, 30]:\n",
    "                    filter_.append([worktype, day]) \n",
    "        else:\n",
    "           for worktype in worktypes: \n",
    "                for day in [1, 2, 3, 4, 5, 6, 7, 30]:\n",
    "                    filter_.append([worktype, day]) \n",
    "    else:\n",
    "        if day_jobs[1] <= 2000:\n",
    "           for worktype in worktypes: \n",
    "                for day in [3, 7, 10, 15, 18, 21, 24, 27, 30]:\n",
    "                    filter_.append([worktype, day])\n",
    "        elif 2000 < day_jobs[1] <= 4000:\n",
    "           for worktype in worktypes: \n",
    "                for day in [1, 3, 7, 10, 14, 18, 22, 26, 30]:\n",
    "                    filter_.append([worktype, day])  \n",
    "        elif 4000 < day_jobs[1] <= 8000:\n",
    "         for worktype in worktypes:   \n",
    "                for day in [1, 3, 5, 7, 10, 15, 20, 25, 30]:\n",
    "                    filter_.append([worktype, day]) \n",
    "        else:\n",
    "           for worktype in worktypes: \n",
    "                for day in [1, 2, 3, 4, 5, 6, 7, 10, 20, 30]:\n",
    "                    filter_.append([worktype, day]) \n",
    "\n",
    "    return filter_        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a78949de-c4ae-4bbf-a1fe-9d030b886773",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_progress(message):\n",
    "    sys.stdout.write(\"\\r\" + message)\n",
    "    sys.stdout.flush()\n",
    "\n",
    "def scrape_this_page(url, current_scraped=None, total_job=None):\n",
    "    '''\n",
    "    Scrape all jobs in this page given an url\n",
    "    '''     \n",
    "    # Try to get job card elements for current page, return None if none  \n",
    "    for try_ in range(8):\n",
    "        response = requests.get(url)\n",
    "        html = BeautifulSoup(response.content, 'html.parser')  \n",
    "        job_card_elements = html.find_all('div', class_='base-search-card--link')\n",
    "        if job_card_elements:\n",
    "            jobs_in_page = len(job_card_elements)\n",
    "            break            \n",
    "        elif try_ == 7:\n",
    "            return None\n",
    "\n",
    "        time.sleep(random.uniform(0.5,1.5))\n",
    "\n",
    "    # Scrape the job information for each job\n",
    "    df_page = pd.DataFrame() \n",
    "    col_name = ['title', 'company', 'location', 'posted_date']\n",
    "    for job in job_card_elements:\n",
    "        total_job += 1\n",
    "        df_job = {}\n",
    "        df_job['title'] = job.find('h3', class_='base-search-card__title').text.strip()\n",
    "        df_job['company'] = job.find('h4', class_='base-search-card__subtitle').text.strip()\n",
    "        location = job.find('span', class_='job-search-card__location')\n",
    "        df_job['location'] = location.text.strip() if location else None        \n",
    "        date_1 = job.find('time', class_='job-search-card__listdate')\n",
    "        date_2 = job.find('time', class_='job-search-card__listdate--new')\n",
    "        df_job['posted_date'] = date_1['datetime'] if date_1 else date_2['datetime']\n",
    "        \n",
    "     \n",
    "        # If job listings already exist, skip\n",
    "        if not df_page.empty:\n",
    "            df_set = set(df_page[col_name].itertuples(index=False, name=None))\n",
    "            if tuple(df_job.values()) in df_set:\n",
    "                if current_scraped != None and total_job != None:\n",
    "                    print_progress(progress_msg1 + f'{current_scraped}/{total_job}  (Saved / Total Jobs)')\n",
    "                continue\n",
    "        if os.path.exists(csv_path):\n",
    "            scraped_df = pd.read_csv(csv_path, usecols=col_name, on_bad_lines='skip')\n",
    "            df_set = set(scraped_df.itertuples(index=False, name=None))\n",
    "            if tuple(df_job.values()) in df_set:\n",
    "                if current_scraped != None and total_job != None:    \n",
    "                    print_progress(progress_msg1 + f'{current_scraped}/{total_job}  (Saved / Total Jobs)')\n",
    "                continue\n",
    "\n",
    "                \n",
    "        df_job['worktype'] = worktype if worktype else None\n",
    "        salary = job.find('span', class_='job-search-card__salary-info')\n",
    "        df_job['salary'] = salary.text.strip() if salary else None\n",
    "        df_job['job_link'] = job.find('a', class_='base-card__full-link')['href']  \n",
    "        \n",
    "        \n",
    "        \n",
    "        # Go inside the job link and scrape job description and other information\n",
    "        # Discard all information about the job if job description is empty\n",
    "        skip = False\n",
    "        for try_ in range(5):\n",
    "            job_response = requests.get(df_job['job_link'])\n",
    "            job_html = BeautifulSoup(job_response.content, 'html.parser')\n",
    "            job_description = job_html.find('div', class_='show-more-less-html__markup')                \n",
    "            if job_description:\n",
    "                break\n",
    "            if try_ == 4:\n",
    "                skip = True\n",
    "                break\n",
    "            \n",
    "            time.sleep(random.uniform(1,3))                          \n",
    "        if skip:\n",
    "            if current_scraped != None and total_job != None:\n",
    "                print_progress(progress_msg1 + f'{current_scraped}/{total_job}  (Saved / Total Jobs)')\n",
    "            continue\n",
    "        df_job['job_description'] = job_description.text\n",
    "                               \n",
    "        applicants_1 = job_html.find('figcaption', class_='num-applicants__caption')\n",
    "        applicants_2 = job_html.find('span', class_='num-applicants__caption')\n",
    "        df_job['applicants'] = applicants_1.text if applicants_1 else applicants_2.text\n",
    "\n",
    "        criteria = job_html.find_all('li', class_='description__job-criteria-item')\n",
    "        for criterion in criteria:\n",
    "            feature = criterion.find('h3', class_='description__job-criteria-subheader').text.strip()\n",
    "            value = criterion.find('span', class_='description__job-criteria-text--criteria').text.strip()\n",
    "            df_job[feature] = value\n",
    "\n",
    "        df_job = pd.DataFrame([df_job]) \n",
    "        df_page = df_job if df_page.empty else pd.concat([df_page, df_job], ignore_index=True)  \n",
    "        current_scraped += 1\n",
    "        if current_scraped != None and total_job != None:\n",
    "            print_progress(progress_msg1 + f'{current_scraped}/{total_job}  (Saved / Total Jobs)')\n",
    "    \n",
    "    return current_scraped, total_job, df_page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a131eae4-a49f-43a5-ab5c-cb217707d74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = 'California'\n",
    "worktype = 'On-site'\n",
    "day = 1\n",
    "def get_start_filter(state=state, worktype=worktype, day=day):\n",
    "    state_index = states.index(state)\n",
    "    worktype_index = ['On-site', 'Remote', 'Hybrid'].index(worktype)\n",
    "    return state_index, worktype_index, day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79dbac72-0bc7-4119-bf36-0f84cf87ba4b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "            ----------------------------------------------------------------------------------------------\n",
      "                                 *****  California State (Business Analyst)  ***** \n",
      "\n",
      "                      Any time (13)                On-site (1)            Mid-Senior level (1)          \n",
      "                    ✓ Past month (2)               Hybrid (1)                                           \n",
      "                      Past week (1)                                                                     \n",
      "                      Past 24 hours (1)                                                                 \n",
      "           ----------------------------------------------------------------------------------------------\n",
      "\n",
      "Data Will Save to 'datasets/business_analyst/california.csv'\n",
      "#####   |  Worktype: On-site, Day: 30  |   #####     1/41  (Saved / Total Jobs)\n",
      "#####   |  Worktype: Remote, Day: 30  |   #####     0/40  (Saved / Total Jobs)\n",
      "#####   |  Worktype: Hybrid, Day: 30  |   #####     1/41  (Saved / Total Jobs)\n",
      "\n",
      "            -----------------------------------------------------------------------------------\n",
      "                                *****  Texas State (Business Analyst)  ***** \n",
      "\n",
      "                      Any time (2)              Hybrid (1)            Associate (1)          \n",
      "                    ✓ Past month (1)                                                         \n",
      "           -----------------------------------------------------------------------------------\n",
      "\n",
      "Data Will Save to 'datasets/business_analyst/texas.csv'\n",
      "#####   |  Worktype: On-site, Day: 30  |   #####     "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 73\u001b[0m\n\u001b[0;32m     71\u001b[0m job_filter\u001b[38;5;241m.\u001b[39mupdate(day\u001b[38;5;241m=\u001b[39mday, worktype\u001b[38;5;241m=\u001b[39mworktype, job_num\u001b[38;5;241m=\u001b[39mtotal_job)\n\u001b[0;32m     72\u001b[0m url \u001b[38;5;241m=\u001b[39m job_filter\u001b[38;5;241m.\u001b[39mcreate_url()\n\u001b[1;32m---> 73\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mscrape_this_page\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_scraped\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_job\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     76\u001b[0m     empty_page \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "Cell \u001b[1;32mIn[5], line 20\u001b[0m, in \u001b[0;36mscrape_this_page\u001b[1;34m(url, current_scraped, total_job)\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m try_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m7\u001b[39m:\n\u001b[0;32m     18\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muniform\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1.5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Scrape the job information for each job\u001b[39;00m\n\u001b[0;32m     23\u001b[0m df_page \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame() \n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Start filter ---------------------------------------------------\n",
    "title = 'Business Analyst' \n",
    "states = get_states()\n",
    "start_filter = get_start_filter('California', 'On-site', 1)\n",
    "# Start filter ---------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "# Create a dataset folder if it doesn't exist\n",
    "if not os.path.exists('datasets'):\n",
    "    os.makedirs('datasets')\n",
    "# Create a title folder if it doesn't exist\n",
    "title_folder = title.lower().replace(' ', '_')\n",
    "if not os.path.exists(f'datasets/{title_folder}'):\n",
    "    os.makedirs(f'datasets/{title_folder}')\n",
    "# Create a JSON file that record job counts for each state\n",
    "json_path = f'datasets/{title_folder}/job_counts.json'\n",
    "if not os.path.exists(json_path):\n",
    "    with open(json_path, 'w') as  json_file:\n",
    "        json.dump({'california': None}, json_file, indent=4)\n",
    "    \n",
    "# Iterate each state\n",
    "for state in states:\n",
    "    \n",
    "    # Start from a specific state -----------------------------------------------\n",
    "    if states.index(state) < start_filter[0]:\n",
    "        continue\n",
    "    # Skip used states -----------------------------------------------\n",
    "\n",
    "    \n",
    "    # Get job counts for this state\n",
    "    location = state\n",
    "    if state != 'District of Columbia':\n",
    "        location = state + ' State'\n",
    "    job_filter = SetFilter(title=title, location=location, day=30)\n",
    "    job_counts = job_filter.show_job_counts()\n",
    "    \n",
    "    # Set the path of csv file         \n",
    "    csv_path = f\"datasets/{title_folder}/{state.lower().replace(' ', '_')}.csv\"\n",
    "    print(f\"\\nData Will Save to '{csv_path}'\")\n",
    "    \n",
    "    # Update job counts data in the JSON file\n",
    "    with open(json_path, 'r') as json_file:\n",
    "        job_counts_json = json.load(json_file)\n",
    "    job_counts_json.update({state.lower().replace(' ', '_'): job_counts})\n",
    "    with open(json_path, 'w') as json_file:\n",
    "        json.dump(job_counts_json, json_file, indent=4)\n",
    "    \n",
    "    filter_iter = decide_scrape_scope(job_counts)\n",
    "    \n",
    "    # Iterate each filter set\n",
    "    for worktype, day in filter_iter:        \n",
    "        \n",
    "        # Skip filters already used--------------------------------------------------------\n",
    "        if states.index(state) < start_filter[0]:\n",
    "            continue\n",
    "        if ['On-site', 'Remote', 'Hybrid'].index(worktype) < start_filter[1]:\n",
    "            continue\n",
    "        if day < start_filter[2]:\n",
    "            continue\n",
    "        # Skip filters already used--------------------------------------------------------\n",
    "        \n",
    "        \n",
    "        # Print Progress Message\n",
    "        progress_msg1 = f\"#####   |  Worktype: {worktype if worktype else 'All'}, Day: {day}  |   #####     \"\n",
    "        print_progress(progress_msg1)\n",
    "        # Iterate each page for this filter to scrape jobs\n",
    "        total_job = current_scraped = empty_page = 0        \n",
    "        while total_job < 1000:\n",
    "            # Navigate to different api pages and scraped job listings by adjusting job_num, maximum 1000\n",
    "            job_filter.update(day=day, worktype=worktype, job_num=total_job)\n",
    "            url = job_filter.create_url()\n",
    "            result = scrape_this_page(url, current_scraped, total_job)\n",
    "            \n",
    "            if result != None:\n",
    "                empty_page = 0\n",
    "                current_scraped, total_job, df_page = result\n",
    "            else:\n",
    "                if empty_page > 3:\n",
    "                    break\n",
    "                empty_page += 1\n",
    "                # Print Progress Message\n",
    "                print_progress(progress_msg1 + f'{current_scraped}/{total_job+10}  (Saved / Total Jobs)')\n",
    "                total_job += 10\n",
    "                continue\n",
    "                \n",
    "\n",
    "            # Create a new csv file if it doesn't exist, else add new data to the file\n",
    "            if not os.path.exists(csv_path):               \n",
    "                df_page.to_csv(f'{csv_path}', index=False)           \n",
    "            else:\n",
    "                if df_page is None:\n",
    "                    continue\n",
    "                elif df_page.empty:\n",
    "                    continue\n",
    "                else:\n",
    "                    df_page.to_csv(f'{csv_path}', mode='a', header=False, index=False)           \n",
    "            \n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c3b0c2-32f2-44cd-8c35-01121c8437c6",
   "metadata": {},
   "source": [
    "## Check the Scraped Jobs and Scraped again if Needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "953c8713-9b6e-495e-956e-f4f3cef5912f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_job_count(job_series):\n",
    "    job_series = job_series.str.replace(',', '')\n",
    "    job_df = job_series.str.extract(r'\\((?P<' + job_series.name + r'>[\\d,]+)\\)')\n",
    "    return job_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "742d545d-174b-4214-92e8-94033add8194",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_day_count(day_list):\n",
    "    day_counts = str(day_list).replace(',', '')\n",
    "    past_month = re.search(r'Past month \\((\\d+)\\)', day_counts)\n",
    "    past_month = str(0) if not past_month else past_month[1]\n",
    "    past_week = re.search(r'Past week \\((\\d+)\\)', day_counts)\n",
    "    past_week = str(0) if not past_week else past_week[1]\n",
    "    past_day = re.search(r'Past 24 hours \\((\\d+)\\)', day_counts) \n",
    "    past_day = str(0) if not past_day else past_day[1]\n",
    "    \n",
    "    return f' {past_day} - {past_week} - {past_month} '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bdc2c2aa-e554-4c40-9f11-2f1b5f7a6417",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Analyst\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yongh\\AppData\\Local\\Temp\\ipykernel_21788\\2766279208.py:35: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  total = df_part.agg('sum', axis=1)[0]\n",
      "C:\\Users\\yongh\\AppData\\Local\\Temp\\ipykernel_21788\\2766279208.py:35: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  total = df_part.agg('sum', axis=1)[0]\n",
      "C:\\Users\\yongh\\AppData\\Local\\Temp\\ipykernel_21788\\2766279208.py:35: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  total = df_part.agg('sum', axis=1)[0]\n",
      "C:\\Users\\yongh\\AppData\\Local\\Temp\\ipykernel_21788\\2766279208.py:35: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  total = df_part.agg('sum', axis=1)[0]\n",
      "C:\\Users\\yongh\\AppData\\Local\\Temp\\ipykernel_21788\\2766279208.py:35: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  total = df_part.agg('sum', axis=1)[0]\n",
      "C:\\Users\\yongh\\AppData\\Local\\Temp\\ipykernel_21788\\2766279208.py:35: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  total = df_part.agg('sum', axis=1)[0]\n",
      "C:\\Users\\yongh\\AppData\\Local\\Temp\\ipykernel_21788\\2766279208.py:35: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  total = df_part.agg('sum', axis=1)[0]\n",
      "C:\\Users\\yongh\\AppData\\Local\\Temp\\ipykernel_21788\\2766279208.py:35: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  total = df_part.agg('sum', axis=1)[0]\n",
      "C:\\Users\\yongh\\AppData\\Local\\Temp\\ipykernel_21788\\2766279208.py:35: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  total = df_part.agg('sum', axis=1)[0]\n",
      "C:\\Users\\yongh\\AppData\\Local\\Temp\\ipykernel_21788\\2766279208.py:35: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  total = df_part.agg('sum', axis=1)[0]\n",
      "C:\\Users\\yongh\\AppData\\Local\\Temp\\ipykernel_21788\\2766279208.py:35: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  total = df_part.agg('sum', axis=1)[0]\n",
      "C:\\Users\\yongh\\AppData\\Local\\Temp\\ipykernel_21788\\2766279208.py:35: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  total = df_part.agg('sum', axis=1)[0]\n",
      "C:\\Users\\yongh\\AppData\\Local\\Temp\\ipykernel_21788\\2766279208.py:35: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  total = df_part.agg('sum', axis=1)[0]\n",
      "C:\\Users\\yongh\\AppData\\Local\\Temp\\ipykernel_21788\\2766279208.py:35: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  total = df_part.agg('sum', axis=1)[0]\n",
      "C:\\Users\\yongh\\AppData\\Local\\Temp\\ipykernel_21788\\2766279208.py:35: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  total = df_part.agg('sum', axis=1)[0]\n",
      "C:\\Users\\yongh\\AppData\\Local\\Temp\\ipykernel_21788\\2766279208.py:35: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  total = df_part.agg('sum', axis=1)[0]\n",
      "C:\\Users\\yongh\\AppData\\Local\\Temp\\ipykernel_21788\\2766279208.py:35: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  total = df_part.agg('sum', axis=1)[0]\n",
      "C:\\Users\\yongh\\AppData\\Local\\Temp\\ipykernel_21788\\2766279208.py:35: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  total = df_part.agg('sum', axis=1)[0]\n",
      "C:\\Users\\yongh\\AppData\\Local\\Temp\\ipykernel_21788\\2766279208.py:35: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  total = df_part.agg('sum', axis=1)[0]\n",
      "C:\\Users\\yongh\\AppData\\Local\\Temp\\ipykernel_21788\\2766279208.py:35: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  total = df_part.agg('sum', axis=1)[0]\n",
      "C:\\Users\\yongh\\AppData\\Local\\Temp\\ipykernel_21788\\2766279208.py:35: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  total = df_part.agg('sum', axis=1)[0]\n",
      "C:\\Users\\yongh\\AppData\\Local\\Temp\\ipykernel_21788\\2766279208.py:35: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  total = df_part.agg('sum', axis=1)[0]\n",
      "C:\\Users\\yongh\\AppData\\Local\\Temp\\ipykernel_21788\\2766279208.py:35: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  total = df_part.agg('sum', axis=1)[0]\n",
      "C:\\Users\\yongh\\AppData\\Local\\Temp\\ipykernel_21788\\2766279208.py:35: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  total = df_part.agg('sum', axis=1)[0]\n",
      "C:\\Users\\yongh\\AppData\\Local\\Temp\\ipykernel_21788\\2766279208.py:35: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  total = df_part.agg('sum', axis=1)[0]\n",
      "C:\\Users\\yongh\\AppData\\Local\\Temp\\ipykernel_21788\\2766279208.py:35: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  total = df_part.agg('sum', axis=1)[0]\n",
      "C:\\Users\\yongh\\AppData\\Local\\Temp\\ipykernel_21788\\2766279208.py:35: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  total = df_part.agg('sum', axis=1)[0]\n",
      "C:\\Users\\yongh\\AppData\\Local\\Temp\\ipykernel_21788\\2766279208.py:35: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  total = df_part.agg('sum', axis=1)[0]\n",
      "C:\\Users\\yongh\\AppData\\Local\\Temp\\ipykernel_21788\\2766279208.py:35: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  total = df_part.agg('sum', axis=1)[0]\n",
      "C:\\Users\\yongh\\AppData\\Local\\Temp\\ipykernel_21788\\2766279208.py:35: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  total = df_part.agg('sum', axis=1)[0]\n",
      "C:\\Users\\yongh\\AppData\\Local\\Temp\\ipykernel_21788\\2766279208.py:35: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  total = df_part.agg('sum', axis=1)[0]\n",
      "C:\\Users\\yongh\\AppData\\Local\\Temp\\ipykernel_21788\\2766279208.py:35: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  total = df_part.agg('sum', axis=1)[0]\n",
      "C:\\Users\\yongh\\AppData\\Local\\Temp\\ipykernel_21788\\2766279208.py:35: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  total = df_part.agg('sum', axis=1)[0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hybrid</th>\n",
       "      <th>onsite</th>\n",
       "      <th>remote</th>\n",
       "      <th>total_jobs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>alabama</th>\n",
       "      <td>144 / 274</td>\n",
       "      <td>442 / 590</td>\n",
       "      <td>48 / 100</td>\n",
       "      <td>634 / 987 @ ( 118 - 373 - 987 )</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>arizona</th>\n",
       "      <td>572 / 529</td>\n",
       "      <td>1085 / 1184</td>\n",
       "      <td>386 / 367</td>\n",
       "      <td>2043 / 2058 @ ( 229 - 718 - 2058 )</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>arkansas</th>\n",
       "      <td>NaN</td>\n",
       "      <td>105 / 368</td>\n",
       "      <td>NaN</td>\n",
       "      <td>105 / 544 @ ( 78 - 186 - 544 )</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>california</th>\n",
       "      <td>4037 / 3089</td>\n",
       "      <td>10956 / 8961</td>\n",
       "      <td>2278 / 1877</td>\n",
       "      <td>17271 / 14257 @ ( 840 - 3937 - 14257 )</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>colorado</th>\n",
       "      <td>523 / 572</td>\n",
       "      <td>1535 / 1544</td>\n",
       "      <td>284 / 259</td>\n",
       "      <td>2342 / 2432 @ ( 195 - 778 - 2432 )</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>connecticut</th>\n",
       "      <td>224 / 270</td>\n",
       "      <td>772 / 784</td>\n",
       "      <td>135 / 131</td>\n",
       "      <td>1131 / 1178 @ ( 138 - 554 - 1178 )</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>delaware</th>\n",
       "      <td>45 / 93</td>\n",
       "      <td>116 / 757</td>\n",
       "      <td>15 / 62</td>\n",
       "      <td>176 / 905 @ ( 138 - 647 - 905 )</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>district_of_columbia</th>\n",
       "      <td>511 / 556</td>\n",
       "      <td>2147 / 1704</td>\n",
       "      <td>273 / 286</td>\n",
       "      <td>2931 / 2588 @ ( 267 - 1169 - 2588 )</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>florida</th>\n",
       "      <td>1369 / 1132</td>\n",
       "      <td>3149 / 2651</td>\n",
       "      <td>638 / 597</td>\n",
       "      <td>5156 / 4324 @ ( 120 - 1586 - 4324 )</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>georgia</th>\n",
       "      <td>1292 / 1214</td>\n",
       "      <td>3024 / 3038</td>\n",
       "      <td>424 / 480</td>\n",
       "      <td>4740 / 4657 @ ( 105 - 2109 - 4657 )</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>idaho</th>\n",
       "      <td>35 / 79</td>\n",
       "      <td>69 / 134</td>\n",
       "      <td>33 / 35</td>\n",
       "      <td>137 / 248 @ ( 3 - 96 - 248 )</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>illinois</th>\n",
       "      <td>1397 / 1385</td>\n",
       "      <td>4031 / 3702</td>\n",
       "      <td>630 / 536</td>\n",
       "      <td>6058 / 5627 @ ( 106 - 2666 - 5627 )</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>indiana</th>\n",
       "      <td>209 / 238</td>\n",
       "      <td>840 / 876</td>\n",
       "      <td>140 / 167</td>\n",
       "      <td>1189 / 1265 @ ( 20 - 545 - 1265 )</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>iowa</th>\n",
       "      <td>74 / 130</td>\n",
       "      <td>140 / 542</td>\n",
       "      <td>33 / 87</td>\n",
       "      <td>247 / 760 @ ( 19 - 333 - 760 )</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kansas</th>\n",
       "      <td>66 / 179</td>\n",
       "      <td>141 / 381</td>\n",
       "      <td>53 / 155</td>\n",
       "      <td>260 / 714 @ ( 3 - 266 - 714 )</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kentucky</th>\n",
       "      <td>31 / 83</td>\n",
       "      <td>84 / 280</td>\n",
       "      <td>26 / 52</td>\n",
       "      <td>141 / 415 @ ( 8 - 158 - 415 )</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>louisiana</th>\n",
       "      <td>40 / 74</td>\n",
       "      <td>67 / 185</td>\n",
       "      <td>44 / 66</td>\n",
       "      <td>151 / 325 @ ( 2 - 100 - 325 )</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>maine</th>\n",
       "      <td>13 / 64</td>\n",
       "      <td>56 / 155</td>\n",
       "      <td>22 / 34</td>\n",
       "      <td>91 / 253 @ ( 2 - 119 - 253 )</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>maryland</th>\n",
       "      <td>1134 / 975</td>\n",
       "      <td>4120 / 4702</td>\n",
       "      <td>362 / 393</td>\n",
       "      <td>5616 / 6035 @ ( 99 - 2973 - 6035 )</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>massachusetts</th>\n",
       "      <td>1146 / 994</td>\n",
       "      <td>2848 / 2540</td>\n",
       "      <td>475 / 524</td>\n",
       "      <td>4469 / 4172 @ ( 185 - 1652 - 4172 )</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>michigan</th>\n",
       "      <td>650 / 730</td>\n",
       "      <td>1966 / 1749</td>\n",
       "      <td>213 / 238</td>\n",
       "      <td>2829 / 2727 @ ( 270 - 1326 - 2727 )</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>minnesota</th>\n",
       "      <td>434 / 389</td>\n",
       "      <td>1002 / 991</td>\n",
       "      <td>331 / 314</td>\n",
       "      <td>1767 / 1710 @ ( 150 - 748 - 1710 )</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mississippi</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>152 / 184 @ ( 13 - 78 - 184 )</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>missouri</th>\n",
       "      <td>414 / 368</td>\n",
       "      <td>839 / 842</td>\n",
       "      <td>217 / 229</td>\n",
       "      <td>1470 / 1412 @ ( 119 - 592 - 1412 )</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>montana</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>120 / 147 @ ( 5 - 35 - 147 )</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nebraska</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>381 / 515 @ ( 37 - 197 - 515 )</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nevada</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>422 / 567 @ ( 49 - 199 - 567 )</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>new_hampshire</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>310 / 445 @ ( 95 - 189 - 445 )</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>new_jersey</th>\n",
       "      <td>1288 / 1201</td>\n",
       "      <td>2892 / 2883</td>\n",
       "      <td>257 / 278</td>\n",
       "      <td>4437 / 4411 @ ( 533 - 1624 - 4411 )</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>new_mexico</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>223 / 311 @ ( 18 - 85 - 311 )</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>new_york</th>\n",
       "      <td>2200 / 1957</td>\n",
       "      <td>6549 / 5234</td>\n",
       "      <td>1037 / 985</td>\n",
       "      <td>9786 / 7811 @ ( 910 - 3080 - 7811 )</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>north_carolina</th>\n",
       "      <td>982 / 962</td>\n",
       "      <td>2256 / 2287</td>\n",
       "      <td>520 / 445</td>\n",
       "      <td>3758 / 3689 @ ( 275 - 1076 - 3689 )</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>north_dakota</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>112 / 132 @ ( 7 - 43 - 132 )</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ohio</th>\n",
       "      <td>744 / 712</td>\n",
       "      <td>2844 / 2626</td>\n",
       "      <td>327 / 341</td>\n",
       "      <td>3915 / 3628 @ ( 1145 - 1850 - 3628 )</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oklahoma</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>426 / 546 @ ( 44 - 179 - 546 )</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oregon</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>669 / 837 @ ( 77 - 281 - 837 )</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pennsylvania</th>\n",
       "      <td>1227 / 1048</td>\n",
       "      <td>2866 / 2699</td>\n",
       "      <td>473 / 460</td>\n",
       "      <td>4566 / 4391 @ ( 385 - 1866 - 4391 )</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>south_carolina</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>615 / 833 @ ( 66 - 324 - 833 )</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>south_dakota</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>122 / 148 @ ( 14 - 57 - 148 )</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tennessee</th>\n",
       "      <td>356 / 279</td>\n",
       "      <td>849 / 817</td>\n",
       "      <td>254 / 214</td>\n",
       "      <td>1459 / 1304 @ ( 99 - 482 - 1304 )</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>texas</th>\n",
       "      <td>3089 / 3054</td>\n",
       "      <td>9828 / 8451</td>\n",
       "      <td>1936 / 1389</td>\n",
       "      <td>14853 / 13189 @ ( 1316 - 5011 - 13189 )</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>utah</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>568 / 878 @ ( 67 - 278 - 878 )</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vermont</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>70 / 103 @ ( 5 - 29 - 103 )</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>virginia</th>\n",
       "      <td>1911 / 1988</td>\n",
       "      <td>7599 / 8709</td>\n",
       "      <td>563 / 599</td>\n",
       "      <td>10073 / 11242 @ ( 1413 - 5453 - 11242 )</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>washington</th>\n",
       "      <td>816 / 828</td>\n",
       "      <td>5242 / 4313</td>\n",
       "      <td>615 / 641</td>\n",
       "      <td>6673 / 5792 @ ( 94 - 1896 - 5792 )</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>west_virginia</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>103 / 119 @ ( 5 - 39 - 119 )</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wisconsin</th>\n",
       "      <td>268 / 232</td>\n",
       "      <td>780 / 676</td>\n",
       "      <td>200 / 182</td>\n",
       "      <td>1248 / 1092 @ ( 22 - 364 - 1092 )</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wyoming</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>68 / 73 @ ( 4 - 23 - 73 )</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           hybrid        onsite       remote  \\\n",
       "alabama                 144 / 274     442 / 590     48 / 100   \n",
       "arizona                 572 / 529   1085 / 1184    386 / 367   \n",
       "arkansas                      NaN     105 / 368          NaN   \n",
       "california            4037 / 3089  10956 / 8961  2278 / 1877   \n",
       "colorado                523 / 572   1535 / 1544    284 / 259   \n",
       "connecticut             224 / 270     772 / 784    135 / 131   \n",
       "delaware                  45 / 93     116 / 757      15 / 62   \n",
       "district_of_columbia    511 / 556   2147 / 1704    273 / 286   \n",
       "florida               1369 / 1132   3149 / 2651    638 / 597   \n",
       "georgia               1292 / 1214   3024 / 3038    424 / 480   \n",
       "idaho                     35 / 79      69 / 134      33 / 35   \n",
       "illinois              1397 / 1385   4031 / 3702    630 / 536   \n",
       "indiana                 209 / 238     840 / 876    140 / 167   \n",
       "iowa                     74 / 130     140 / 542      33 / 87   \n",
       "kansas                   66 / 179     141 / 381     53 / 155   \n",
       "kentucky                  31 / 83      84 / 280      26 / 52   \n",
       "louisiana                 40 / 74      67 / 185      44 / 66   \n",
       "maine                     13 / 64      56 / 155      22 / 34   \n",
       "maryland               1134 / 975   4120 / 4702    362 / 393   \n",
       "massachusetts          1146 / 994   2848 / 2540    475 / 524   \n",
       "michigan                650 / 730   1966 / 1749    213 / 238   \n",
       "minnesota               434 / 389    1002 / 991    331 / 314   \n",
       "mississippi                   NaN           NaN          NaN   \n",
       "missouri                414 / 368     839 / 842    217 / 229   \n",
       "montana                       NaN           NaN          NaN   \n",
       "nebraska                      NaN           NaN          NaN   \n",
       "nevada                        NaN           NaN          NaN   \n",
       "new_hampshire                 NaN           NaN          NaN   \n",
       "new_jersey            1288 / 1201   2892 / 2883    257 / 278   \n",
       "new_mexico                    NaN           NaN          NaN   \n",
       "new_york              2200 / 1957   6549 / 5234   1037 / 985   \n",
       "north_carolina          982 / 962   2256 / 2287    520 / 445   \n",
       "north_dakota                  NaN           NaN          NaN   \n",
       "ohio                    744 / 712   2844 / 2626    327 / 341   \n",
       "oklahoma                      NaN           NaN          NaN   \n",
       "oregon                        NaN           NaN          NaN   \n",
       "pennsylvania          1227 / 1048   2866 / 2699    473 / 460   \n",
       "south_carolina                NaN           NaN          NaN   \n",
       "south_dakota                  NaN           NaN          NaN   \n",
       "tennessee               356 / 279     849 / 817    254 / 214   \n",
       "texas                 3089 / 3054   9828 / 8451  1936 / 1389   \n",
       "utah                          NaN           NaN          NaN   \n",
       "vermont                       NaN           NaN          NaN   \n",
       "virginia              1911 / 1988   7599 / 8709    563 / 599   \n",
       "washington              816 / 828   5242 / 4313    615 / 641   \n",
       "west_virginia                 NaN           NaN          NaN   \n",
       "wisconsin               268 / 232     780 / 676    200 / 182   \n",
       "wyoming                       NaN           NaN          NaN   \n",
       "\n",
       "                                                   total_jobs  \n",
       "alabama                       634 / 987 @ ( 118 - 373 - 987 )  \n",
       "arizona                    2043 / 2058 @ ( 229 - 718 - 2058 )  \n",
       "arkansas                       105 / 544 @ ( 78 - 186 - 544 )  \n",
       "california             17271 / 14257 @ ( 840 - 3937 - 14257 )  \n",
       "colorado                   2342 / 2432 @ ( 195 - 778 - 2432 )  \n",
       "connecticut                1131 / 1178 @ ( 138 - 554 - 1178 )  \n",
       "delaware                      176 / 905 @ ( 138 - 647 - 905 )  \n",
       "district_of_columbia      2931 / 2588 @ ( 267 - 1169 - 2588 )  \n",
       "florida                   5156 / 4324 @ ( 120 - 1586 - 4324 )  \n",
       "georgia                   4740 / 4657 @ ( 105 - 2109 - 4657 )  \n",
       "idaho                            137 / 248 @ ( 3 - 96 - 248 )  \n",
       "illinois                  6058 / 5627 @ ( 106 - 2666 - 5627 )  \n",
       "indiana                     1189 / 1265 @ ( 20 - 545 - 1265 )  \n",
       "iowa                           247 / 760 @ ( 19 - 333 - 760 )  \n",
       "kansas                          260 / 714 @ ( 3 - 266 - 714 )  \n",
       "kentucky                        141 / 415 @ ( 8 - 158 - 415 )  \n",
       "louisiana                       151 / 325 @ ( 2 - 100 - 325 )  \n",
       "maine                            91 / 253 @ ( 2 - 119 - 253 )  \n",
       "maryland                   5616 / 6035 @ ( 99 - 2973 - 6035 )  \n",
       "massachusetts             4469 / 4172 @ ( 185 - 1652 - 4172 )  \n",
       "michigan                  2829 / 2727 @ ( 270 - 1326 - 2727 )  \n",
       "minnesota                  1767 / 1710 @ ( 150 - 748 - 1710 )  \n",
       "mississippi                     152 / 184 @ ( 13 - 78 - 184 )  \n",
       "missouri                   1470 / 1412 @ ( 119 - 592 - 1412 )  \n",
       "montana                          120 / 147 @ ( 5 - 35 - 147 )  \n",
       "nebraska                       381 / 515 @ ( 37 - 197 - 515 )  \n",
       "nevada                         422 / 567 @ ( 49 - 199 - 567 )  \n",
       "new_hampshire                  310 / 445 @ ( 95 - 189 - 445 )  \n",
       "new_jersey                4437 / 4411 @ ( 533 - 1624 - 4411 )  \n",
       "new_mexico                      223 / 311 @ ( 18 - 85 - 311 )  \n",
       "new_york                  9786 / 7811 @ ( 910 - 3080 - 7811 )  \n",
       "north_carolina            3758 / 3689 @ ( 275 - 1076 - 3689 )  \n",
       "north_dakota                     112 / 132 @ ( 7 - 43 - 132 )  \n",
       "ohio                     3915 / 3628 @ ( 1145 - 1850 - 3628 )  \n",
       "oklahoma                       426 / 546 @ ( 44 - 179 - 546 )  \n",
       "oregon                         669 / 837 @ ( 77 - 281 - 837 )  \n",
       "pennsylvania              4566 / 4391 @ ( 385 - 1866 - 4391 )  \n",
       "south_carolina                 615 / 833 @ ( 66 - 324 - 833 )  \n",
       "south_dakota                    122 / 148 @ ( 14 - 57 - 148 )  \n",
       "tennessee                   1459 / 1304 @ ( 99 - 482 - 1304 )  \n",
       "texas                 14853 / 13189 @ ( 1316 - 5011 - 13189 )  \n",
       "utah                           568 / 878 @ ( 67 - 278 - 878 )  \n",
       "vermont                           70 / 103 @ ( 5 - 29 - 103 )  \n",
       "virginia              10073 / 11242 @ ( 1413 - 5453 - 11242 )  \n",
       "washington                 6673 / 5792 @ ( 94 - 1896 - 5792 )  \n",
       "west_virginia                    103 / 119 @ ( 5 - 39 - 119 )  \n",
       "wisconsin                   1248 / 1092 @ ( 22 - 364 - 1092 )  \n",
       "wyoming                             68 / 73 @ ( 4 - 23 - 73 )  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ------------------------------------------------------------------------------- \n",
    "title = 'Data Analyst'\n",
    "print(title)\n",
    "title = title.lower().replace(' ', '_')\n",
    "# ------------------------------------------------------------------------------- \n",
    "\n",
    "\n",
    "json_path = f'datasets/{title}/job_counts.json'\n",
    "# Get target scraping jobs from json\n",
    "df = pd.read_json(json_path).T.sort_index()\n",
    "df_total = df['day_element'].apply(lambda lst: lst[1])\n",
    "df_total = extract_job_count(df_total)\n",
    "df_day = df['day_element'].apply(lambda lst: extract_day_count(lst[1:]))\n",
    "df_worktype = df['worktype_element'].explode()\n",
    "df_worktype = extract_job_count(df_worktype)\n",
    "df_worktype = df_worktype.groupby(df_worktype.index)['worktype_element'].apply(lambda x: pd.Series(x.values)).unstack()\n",
    "df_target = pd.concat([df_total, df_worktype], axis=1)\n",
    "df_target.columns = ['total_jobs', 'onsite', 'hybrid', 'remote']\n",
    "\n",
    "# Get already scraped jobs from the csv file\n",
    "df_scraped = pd.DataFrame()\n",
    "for csv_path in os.listdir(f'datasets/{title}'):\n",
    "    if csv_path == 'job_counts.json':\n",
    "        continue\n",
    "    df = pd.read_csv(f'datasets/{title}/{csv_path}', usecols=['worktype'])\n",
    "    df_part = df.value_counts().to_frame()\n",
    "    df_part.index.names = [None]\n",
    "    flat_index = df_part.index.get_level_values(0)\n",
    "    flat_index = pd.Index([index.lower().replace('-', '') for index in flat_index])\n",
    "    df_part = df_part.set_index(flat_index).T\n",
    "    df_part.set_index(pd.Index([csv_path[:-4]]), inplace=True)\n",
    "    if df_part.empty:\n",
    "        total = len(df)\n",
    "    else:\n",
    "        total = df_part.agg('sum', axis=1)[0]\n",
    "    \n",
    "    df_part['total_jobs'] = total \n",
    "    df_part = df_part.astype(str)\n",
    "    df_scraped = df_part if df_scraped.empty else pd.concat([df_scraped, df_part])\n",
    "    \n",
    "df_mid = df_scraped.copy()\n",
    "df_mid[:] = ' / '\n",
    "df_result = df_scraped + df_mid + df_target\n",
    "df_result['total_jobs'] = df_result['total_jobs'] + ' @ (' + df_day + ')'\n",
    "df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "966a5417-beb5-48c7-a99a-380f1239dee8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_only_filter():\n",
    "    return [\n",
    "            # ('Alabama', 7, 'On-site'), \n",
    "            # ('Arizona', 7, 'On-site'), ('Arizona', 7, 'Remote'),\n",
    "            # ('Arkansas', 30, 'On-site'),\n",
    "            # ('California', 1, 'On-site'), ('California', 2, 'On-site'), ('California', 3, 'On-site'),\n",
    "            # ('California', 4, 'On-site'), ('California', 5, 'On-site'), ('California', 6, 'On-site'),\n",
    "            # ('California', 7, 'On-site'), #('California', 7, 'On-site'), ('California', 7, 'On-site'),\n",
    "            # ('California', 14, 'On-site'), ('California', 21, 'On-site'), ('California', 28, 'On-site'),\n",
    "            # ('California', 30, 'On-site'), ('California', 30, 'On-site'), ('California', 30, 'On-site'),\n",
    "            # ('California', 7, 'Hybrid'), ('California', 7, 'Remote'),\n",
    "            # ('Colorado', 7, 'On-site'), ('Colorado', 7, 'Remote'), ('Colorado', 7, 'Hybrid'),\n",
    "            # ('Connecticut', 30, 'On-site'), ('Delaware', 30, 'On-site'),\n",
    "            # ('District of Columbia', 7, 'On-site'), #('District of Columbia', 30, 'On-site'),\n",
    "            # ('District of Columbia', 7, 'Hybrid'), ('District of Columbia', 30, 'Remote'),\n",
    "            # ('Florida', 7, 'On-site'), ('Florida', 7, 'Hybrid'), #('Florida', 21, 'On-site'),\n",
    "            # ('Georgia', 7, 'On-site'), #('Georgia', 7, 'On-site'), #('Georgia', 21, 'On-site'),\n",
    "            # ('Georgia', 7, 'Remote'), ('Georgia', 7, 'Hybrid'), \n",
    "            # ('Idaho', 7, 'Remote'), ('Idaho', 7, 'On-site'), \n",
    "            # ('Illinois', 1, 'On-site'), ('Illinois', 3, 'On-site'), ('Illinois', 7, 'On-site'),\n",
    "            # ('Indiana', 7, 'On-site'), ('Iowa', 7, 'On-site'), \n",
    "            # ('Kansas', 7, 'Hybrid'),\n",
    "            # ('Kentucky', 7, 'On-site'), ('Kentucky', 7, 'Hybrid'), ('Kentucky', 7, 'Remote'),\n",
    "            # ('Louisiana', 7, 'On-site'), ('Louisiana', 7, 'Hybrid'), ('Maryland', 7, 'Hybrid'),\n",
    "            # ('Maryland', 1, 'On-site'), ('Maryland', 2, 'On-site'), ('Maryland', 3, 'On-site'),\n",
    "            # ('Maryland', 4, 'On-site'), ('Maryland', 5, 'On-site'), ('Maryland', 7, 'On-site'),\n",
    "            # ('Maryland', 14, 'On-site'), ('Maryland', 21, 'On-site'), ('Maryland', 30, 'On-site'),\n",
    "            # ('Massachusetts', 7, 'On-site'), ('Massachusetts', 3, 'On-site'), ('Massachusetts', 7, 'On-site'), \n",
    "            # ('Massachusetts', 7, 'Hybrid'), #('Massachusetts', 14, 'Hybrid'),\n",
    "            # ('Michigan', 7, 'On-site'), ('Michigan', 7, 'Hybrid'), #('Michigan', 14, 'On-site'), \n",
    "            # ('Michigan', 7, 'Remote'), \n",
    "            # ('Minnesota', 7, 'Remote'), #('Minnesota', 7, 'Hybrid'), ('Mississippi', 7, 'On-site'),\n",
    "            # ('Missouri', 7, 'Hybrid'), #('Missouri', 30, 'Hybrid'), ('Nebraska', 30, 'On-site'),\n",
    "            # ('Nevada', 7, 'On-site'), #('New Hampshire', 7, 'On-site'),\n",
    "            # ('New Jersey', 7, 'On-site'), ('New Jersey', 14, 'On-site'), ('New Jersey', 21, 'On-site'),\n",
    "            # ('New Jersey', 7, 'Hybrid'),    \n",
    "            # ('New Mexico', 7, 'On-site'), ('New Mexico', 7, 'Hybrid'), \n",
    "            # ('New York', 1, 'On-site'), ('New York', 2, 'On-site'), ('New York', 3, 'On-site'),\n",
    "            # ('New York', 5, 'On-site'), ('New York', 7, 'On-site'), ('New York', 14, 'On-site'),\n",
    "            # ('New York', 21, 'On-site'), ('New York', 30, 'On-site'), #('New York', 30, 'On-site'),\n",
    "            # ('New York', 7, 'Remote'), #('New York', 14, 'Hybrid'),        \n",
    "            # ('North Carolina', 7, 'On-site'), #('North Carolina', 14, 'On-site'), ('North Carolina', 21, 'On-site'),\n",
    "            # ('North Carolina', 7, 'Hybrid'), ('North Carolina', 7, 'Remote'),\n",
    "            # ('North Dakota', 7, 'On-site'), ('North Dakota', 7, 'Hybrid'), ('North Dakota', 7, 'Remote'),\n",
    "            # ('Ohio', 7, 'On-site'), #('Ohio', 14, 'On-site'),('Ohio', 21, 'On-site'),\n",
    "            # ('Ohio', 7, 'Hybrid'), ('Ohio', 14, 'Remote'),\n",
    "            # ('Oklahoma', 30, 'On-site'), ('Oklahoma', 30, 'Remote'),\n",
    "            # ('Oklahoma', 7, 'Hybrid'),\n",
    "            # ('Oregon', 7, 'On-site'), #('Oregon', 30, 'Remote'),\n",
    "            # ('Pennsylvania', 1, 'On-site'), ('Pennsylvania', 7, 'On-site'), ('Pennsylvania', 21, 'On-site'),\n",
    "            # ('Pennsylvania', 7, 'Hybrid'), ('Pennsylvania', 7, 'Remote'),\n",
    "            # ('South Carolina', 7, 'Remote'), #('South Carolina', 7, 'On-site'), ('South Carolina', 30, 'Hybrid'),\n",
    "            # ('South Dakota', 7, 'On-site'), ('South Dakota', 7, 'Hybrid'),\n",
    "            # ('Tennessee', 7, 'On-site'), ('Tennessee', 7, 'Hybrid'), #('Tennessee', 30, 'Remote'),\n",
    "            # ('Texas', 1, 'On-site'), ('Texas', 2, 'On-site'), ('Texas', 3, 'On-site'),\n",
    "            # ('Texas', 4, 'On-site'), ('Texas', 5, 'On-site'), ('Texas', 6, 'On-site'),\n",
    "            # ('Texas', 7, 'On-site'), ('Texas', 14, 'On-site'), ('Texas', 21, 'On-site'),\n",
    "            # ('Texas', 30, 'On-site'), #('Texas', 30, 'On-site'), ('Texas', 30, 'On-site'),\n",
    "            # ('Texas', 7, 'Hybrid'), ('Texas', 7, 'Remote'), ('Texas', 21, 'Hybrid'),\n",
    "            # ('Utah', 7, 'Remote'), #('Utah', 30, 'Hybrid'),\n",
    "            # ('Virginia', 1, 'On-site'), ('Virginia', 2, 'On-site'), ('Virginia', 3, 'On-site'),\n",
    "            # ('Virginia', 4, 'On-site'), ('Virginia', 5, 'On-site'), ('Virginia', 6, 'On-site'), \n",
    "            # ('Virginia', 7, 'On-site'), ('Virginia', 14, 'On-site'), ('Virginia', 21, 'On-site'),\n",
    "            # ('Virginia', 30, 'On-site'), #('Virginia', 30, 'On-site'), ('Virginia', 30, 'On-site'),\n",
    "            # ('Virginia', 7, 'Hybrid'), ('Virginia', 14, 'Hybrid'), ('Virginia', 14, 'Remote'),\n",
    "            ('Washington', 1, 'On-site'), ('Washington', 3, 'On-site'), ('Washington', 7, 'On-site'),\n",
    "            ('Washington', 10, 'On-site'), ('Washington', 12, 'On-site'), ('Washington', 14, 'On-site'),\n",
    "            # ('Washington', 21, 'On-site'), ('Washington', 30, 'On-site'), ('Washington', 30, 'On-site')\n",
    "            # ('Washington', 7, 'Hybrid'), ('Washington', 7, 'Remote'), #('Washington', 14, 'Remote'),\n",
    "            # ('West Virginia', 7, 'On-site'), ('West Virginia', 7, 'Hybrid'), ('West Virginia', 7, 'Remote')\n",
    "            # ('Wisconsin', 7, 'On-site') ('Wisconsin', 30, 'Hybrid'), ('Wisconsin', 30, 'Remote'),\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d1dbd85a-3d07-4e58-8c3c-f853f71411ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "            --------------------------------------------------------------------------------------------------------\n",
      "                                       *****  Washington State (Data Analyst)  ***** \n",
      "\n",
      "                      Any time (9,449)               On-site (3,512)            Internship (37)                   \n",
      "                    ✓ Past month (4,943)             Hybrid (788)               Entry level (575)                 \n",
      "                      Past week (1,832)              Remote (651)               Associate (91)                    \n",
      "                      Past 24 hours (405)                                       Mid-Senior level (2,731)          \n",
      "                                                                                Director (18)                     \n",
      "           --------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Save Data to datasets/data_analyst/washington.csv\n",
      "#####   |Day: 1, Worktype: On-site|   #####     38/323  (Saved / Total Jobs)\n",
      "#####   |Day: 3, Worktype: On-site|   #####     309/765  (Saved / Total Jobs)\n",
      "#####   |Day: 7, Worktype: On-site|   #####     157/1000  (Saved / Total Jobs)\n",
      "#####   |Day: 10, Worktype: On-site|   #####     56/1000  (Saved / Total Jobs)\n",
      "#####   |Day: 12, Worktype: On-site|   #####     30/1000  (Saved / Total Jobs)\n",
      "#####   |Day: 14, Worktype: On-site|   #####     7/1000  (Saved / Total Jobs)\n"
     ]
    }
   ],
   "source": [
    "# Start filter ---------------------------------------------------\n",
    "only_filter = get_only_filter()\n",
    "# Start filter ---------------------------------------------------\n",
    "\n",
    "    \n",
    "# Iterate each filter\n",
    "last_state = None\n",
    "for state, day, worktype in only_filter:\n",
    "    csv_path = f\"datasets/{title}/{state.lower().replace(' ', '_')}.csv\"\n",
    "    # Get job counts for this state\n",
    "    location = state\n",
    "    if state != 'District of Columbia':\n",
    "        location = state + ' State'\n",
    "    job_filter = SetFilter(title=title.replace('_', ' ').title(), location=location, day=30)\n",
    "    if state == last_state:\n",
    "        pass\n",
    "    else:\n",
    "        job_counts = job_filter.show_job_counts()\n",
    "        print(f'\\nSave Data to {csv_path}')\n",
    "        last_state = state\n",
    "    \n",
    "    # Print Progress Message\n",
    "    progress_msg1 = f\"#####   |Day: {day}, Worktype: {worktype if worktype else 'All'}|   #####     \"\n",
    "    print_progress(progress_msg1)\n",
    "        \n",
    "\n",
    "    # Iterate each page for this filter to scrape jobs\n",
    "    total_job = current_scraped = empty_page = 0        \n",
    "    while total_job < 1000:\n",
    "        # Navigate to different api pages and scraped job listings by adjusting job_num, maximum 1000\n",
    "        job_filter.update(day=day, worktype=worktype, job_num=total_job)\n",
    "        url = job_filter.create_url()\n",
    "        result = scrape_this_page(url, current_scraped, total_job)\n",
    "\n",
    "        if result != None:\n",
    "            empty_page = 0\n",
    "            current_scraped, total_job, df_page = result\n",
    "        else:\n",
    "            if empty_page > 3:\n",
    "                break\n",
    "            empty_page += 1\n",
    "            # Print Progress Message\n",
    "            print_progress(progress_msg1 + f'{current_scraped}/{total_job+10}  (Saved / Total Jobs)')\n",
    "            total_job += 10\n",
    "            continue\n",
    "            \n",
    "        # Create a new csv file if it doesn't exist, else add new data to the file\n",
    "        if not os.path.exists(csv_path):               \n",
    "            df_page.to_csv(f'{csv_path}', index=False)           \n",
    "        else:\n",
    "            if df_page is None:\n",
    "                continue\n",
    "            elif df_page.empty:\n",
    "                continue\n",
    "            else:\n",
    "                df_page.to_csv(f'{csv_path}', mode='a', header=False, index=False)           \n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83602b15-59cb-48e7-8896-f9bc0120b290",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29576927-7dab-41d8-afdd-1dc8283125a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ca29c4-b650-401e-a292-2154fb1929ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4047e66a-d017-47be-a799-4a2f7fad8c1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faffb4f9-4bee-4a8a-8873-2efb8380bbed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "82d44657-004c-467b-9a6f-48c003843b63",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yongh\\AppData\\Local\\Temp\\ipykernel_33624\\1518430829.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_fix.drop_duplicates(subset=['title', 'company', 'location', 'posted_date'], keep='last', inplace=True)\n",
      "C:\\Users\\yongh\\AppData\\Local\\Temp\\ipykernel_33624\\1518430829.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_fix.drop_duplicates(subset=['title', 'company', 'location', 'posted_date'], keep='last', inplace=True)\n",
      "C:\\Users\\yongh\\AppData\\Local\\Temp\\ipykernel_33624\\1518430829.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_fix.drop_duplicates(subset=['title', 'company', 'location', 'posted_date'], keep='last', inplace=True)\n",
      "C:\\Users\\yongh\\AppData\\Local\\Temp\\ipykernel_33624\\1518430829.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_fix.drop_duplicates(subset=['title', 'company', 'location', 'posted_date'], keep='last', inplace=True)\n",
      "C:\\Users\\yongh\\AppData\\Local\\Temp\\ipykernel_33624\\1518430829.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_fix.drop_duplicates(subset=['title', 'company', 'location', 'posted_date'], keep='last', inplace=True)\n",
      "C:\\Users\\yongh\\AppData\\Local\\Temp\\ipykernel_33624\\1518430829.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_fix.drop_duplicates(subset=['title', 'company', 'location', 'posted_date'], keep='last', inplace=True)\n",
      "C:\\Users\\yongh\\AppData\\Local\\Temp\\ipykernel_33624\\1518430829.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_fix.drop_duplicates(subset=['title', 'company', 'location', 'posted_date'], keep='last', inplace=True)\n",
      "C:\\Users\\yongh\\AppData\\Local\\Temp\\ipykernel_33624\\1518430829.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_fix.drop_duplicates(subset=['title', 'company', 'location', 'posted_date'], keep='last', inplace=True)\n",
      "C:\\Users\\yongh\\AppData\\Local\\Temp\\ipykernel_33624\\1518430829.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_fix.drop_duplicates(subset=['title', 'company', 'location', 'posted_date'], keep='last', inplace=True)\n",
      "C:\\Users\\yongh\\AppData\\Local\\Temp\\ipykernel_33624\\1518430829.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_fix.drop_duplicates(subset=['title', 'company', 'location', 'posted_date'], keep='last', inplace=True)\n",
      "C:\\Users\\yongh\\AppData\\Local\\Temp\\ipykernel_33624\\1518430829.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_fix.drop_duplicates(subset=['title', 'company', 'location', 'posted_date'], keep='last', inplace=True)\n",
      "C:\\Users\\yongh\\AppData\\Local\\Temp\\ipykernel_33624\\1518430829.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_fix.drop_duplicates(subset=['title', 'company', 'location', 'posted_date'], keep='last', inplace=True)\n",
      "C:\\Users\\yongh\\AppData\\Local\\Temp\\ipykernel_33624\\1518430829.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_fix.drop_duplicates(subset=['title', 'company', 'location', 'posted_date'], keep='last', inplace=True)\n",
      "C:\\Users\\yongh\\AppData\\Local\\Temp\\ipykernel_33624\\1518430829.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_fix.drop_duplicates(subset=['title', 'company', 'location', 'posted_date'], keep='last', inplace=True)\n",
      "C:\\Users\\yongh\\AppData\\Local\\Temp\\ipykernel_33624\\1518430829.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_fix.drop_duplicates(subset=['title', 'company', 'location', 'posted_date'], keep='last', inplace=True)\n",
      "C:\\Users\\yongh\\AppData\\Local\\Temp\\ipykernel_33624\\1518430829.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_fix.drop_duplicates(subset=['title', 'company', 'location', 'posted_date'], keep='last', inplace=True)\n"
     ]
    }
   ],
   "source": [
    "for csv_path in os.listdir('datasets'):\n",
    "    \n",
    "    df_wrong = pd.read_csv(f'datasets/{csv_path}')\n",
    "    is_wrong_row = df_wrong['posted_date'].str.contains(r'On-site|Hybrid|Remote')\n",
    "    if is_wrong_row.sum() > 0:    \n",
    "        df_wrong.loc[is_wrong_row, 'worktype'], df_wrong.loc[is_wrong_row, 'posted_date'] = \\\n",
    "            df_wrong.loc[is_wrong_row, 'posted_date'], df_wrong.loc[is_wrong_row, 'worktype']\n",
    "\n",
    "    cols = list(df_wrong.columns)\n",
    "    worktype_index = cols.index('worktype')\n",
    "    date_index = cols.index('posted_date')\n",
    "    cols[worktype_index], cols[date_index] = cols[date_index], cols[worktype_index]\n",
    "    df_fix = df_wrong[cols]\n",
    "    df_fix.drop_duplicates(subset=['title', 'company', 'location', 'posted_date'], keep='last', inplace=True)\n",
    "    df_fix.to_csv(f'datasets/{csv_path}', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903ec59f-7a4a-4f14-8f5a-84781f6293b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4b3e6f-f72f-4bec-aaf3-2be24582cd10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3de9a9-8c58-4743-b6f3-d2c8e42eb7b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115b1902-d4d7-4365-b977-c72d0942732c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ec9701ac-1559-49f7-a3a5-93d4ff10ac1e",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "35b424a7-e5f3-432f-9d36-0c2264d1078e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def scrape_this_page(url):\n",
    "    '''\n",
    "    Scrape all jobs in this page given an url\n",
    "    '''     \n",
    "    # Try to get job card elements for current page, return None if none  \n",
    "    for try_ in range(5):\n",
    "        response = requests.get(url)\n",
    "        html = BeautifulSoup(response.content, 'html.parser')  \n",
    "        job_card_elements = html.find_all('div', class_='base-search-card--link')\n",
    "        if job_card_elements:\n",
    "            jobs_in_page = len(job_card_elements)\n",
    "            break            \n",
    "        elif try_ == 4:\n",
    "            print(f'  *** EMPTY PAGE ***', end='')\n",
    "            return 0, 0, None\n",
    "        if try_ < 3:\n",
    "            time.sleep(random.randint(3,9))\n",
    "        else:\n",
    "            time.sleep(random.randint(15,30))\n",
    "\n",
    "    # Scrape the job information for each job\n",
    "    df_page = pd.DataFrame()\n",
    "    job_scraped = 0\n",
    "    for job in job_card_elements:\n",
    "        df_job = {}\n",
    "        df_job['title'] = job.find('h3', class_='base-search-card__title').text.strip()\n",
    "        df_job['company'] = job.find('h4', class_='base-search-card__subtitle').text.strip()\n",
    "        df_job['location'] = job.find('span', class_='job-search-card__location').text.strip()\n",
    "        df_job['worktype'] = worktype if worktype else None\n",
    "        date_1 = job.find('time', class_='job-search-card__listdate')\n",
    "        date_2 = job.find('time', class_='job-search-card__listdate--new')\n",
    "        df_job['posted_date'] = date_1['datetime'] if date_1 else date_2['datetime']            \n",
    "        salary = job.find('span', class_='job-search-card__salary-info')\n",
    "        df_job['salary'] = salary.text.strip() if salary else None\n",
    "        df_job['job_link'] = job.find('a', class_='base-card__full-link')['href']            \n",
    "\n",
    "        # Go inside the job link and scrape job description and other information\n",
    "        # Discard all information about the job if job description is empty\n",
    "        skip = False\n",
    "        for try_ in range(5):\n",
    "            job_response = requests.get(df_job['job_link'])\n",
    "            job_html = BeautifulSoup(job_response.content, 'html.parser')\n",
    "            job_description = job_html.find('div', class_='show-more-less-html__markup')                \n",
    "            if job_description:\n",
    "                break\n",
    "            if try_ == 4:\n",
    "                skip = True\n",
    "                break\n",
    "            time.sleep(random.randint(1,5))                               \n",
    "        if skip:\n",
    "            continue\n",
    "        df_job['job_description'] = job_description.text\n",
    "                               \n",
    "        applicants_1 = job_html.find('figcaption', class_='num-applicants__caption')\n",
    "        applicants_2 = job_html.find('span', class_='num-applicants__caption')\n",
    "        df_job['applicants'] = applicants_1.text if applicants_1 else applicants_2.text\n",
    "\n",
    "        criteria = job_html.find_all('li', class_='description__job-criteria-item')\n",
    "        for criterion in criteria:\n",
    "            feature = criterion.find('h3', class_='description__job-criteria-subheader').text.strip()\n",
    "            value = criterion.find('span', class_='description__job-criteria-text--criteria').text.strip()\n",
    "            df_job[feature] = value\n",
    "\n",
    "        df_job = pd.DataFrame([df_job]) \n",
    "        df_page = df_job if df_page.empty else pd.concat([df_page, df_job], ignore_index=True)  \n",
    "        job_scraped += 1\n",
    "        print('.', end='')\n",
    "    \n",
    "    return job_scraped, jobs_in_page, df_page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50f0b67-20a8-40ca-b315-b03e1ab16e44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c5f82e-7033-45ed-accb-1755e7ac8e52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8e883e8f-987d-4b48-ab6c-7c90430fad91",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "            --------------------------------------------------------------------------------------------------------\n",
      "                                              *****  Washington State  ***** \n",
      "\n",
      "                      Any time (9,004)               On-site (4,299)            Internship (38)                   \n",
      "                    ✓ Past month (5,999)             Hybrid (911)               Entry level (691)                 \n",
      "                      Past week (1,693)              Remote (823)               Associate (155)                   \n",
      "                      Past 24 hours (333)                                       Mid-Senior level (3,432)          \n",
      "                                                                                Director (38)                     \n",
      "           --------------------------------------------------------------------------------------------------------\n",
      "\n",
      "#####   |Day: 7, Worktype: On-site|   #####     ..................................................50/60 ..........60/70 ........68/80   *** EMPTY PAGE ***68/85+10 ........76/95 ..........86/105 ........94/115 ........102/125 .......109/135 .........118/145 ..........128/155 .....133/165 .......140/175 ..........150/185 ......156/195 .......163/205 ........171/215 .......178/225 .......185/235 ..187/244 ........195/254 .......202/264 ........210/274 .........219/284 ..........229/294 ..........239/304 ........247/314 ........255/324 .........264/334 .........273/344 ........281/354 ........289/364 .........298/374 .......305/384 .......312/394 .........321/404 ........329/414 .......336/424 ........344/434 .........353/444 ........361/454 .......368/464 .........377/474 .........386/484 .......393/494 ..........403/504 ........411/514 .........420/524 ..........430/534 .......437/544 .........446/554 ........454/564 ..........464/574 ........472/584 ......478/594 ......484/604 ........492/614 .........501/624 ........509/634 .......516/644 .......523/654 ..........533/664   *** EMPTY PAGE ***533/669+10 ......539/679   *** EMPTY PAGE ***539/684+10 .......546/694   *** EMPTY PAGE ***546/699+10 .........555/709 ........563/719 ....."
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_33624\\1895643201.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[0mjob_filter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mday\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mday\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mworktype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworktype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjob_num\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_job\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m             \u001b[0murl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjob_filter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_url\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscrape_this_page\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_33624\\3063429143.py\u001b[0m in \u001b[0;36mscrape_this_page\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m     48\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtry_\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m                 \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m                 \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "title = 'Data Analyst' \n",
    "states = get_states()\n",
    "\n",
    "# Get used filters ---------------------------------------------------\n",
    "used_filter = get_used_filter()\n",
    "# Get used filters ---------------------------------------------------\n",
    "\n",
    "# Create a dataset file if it doesn't exist\n",
    "dataset_folder = 'datasets'\n",
    "if not os.path.exists(dataset_folder):\n",
    "    os.makedirs(dataset_folder)\n",
    "    \n",
    "# Iterate each state\n",
    "for state in states:\n",
    "    \n",
    "    # Skip used states -----------------------------------------------\n",
    "    if state in used_filter:\n",
    "        continue\n",
    "    # Skip used states -----------------------------------------------\n",
    "    \n",
    "    # Create a State file if it doesn't exist\n",
    "    state_folder = state.lower().replace(' ', '_')\n",
    "    if not os.path.exists(f'{dataset_folder}/{state_folder}'):\n",
    "        os.makedirs(f'{dataset_folder}/{state_folder}')\n",
    "    \n",
    "    location = state\n",
    "    if state != 'District of Columbia':\n",
    "        location = state + ' State'\n",
    "    job_filter = SetFilter(title=title, location=location, day=30)\n",
    "    job_counts = job_filter.show_job_counts()\n",
    "    filter_iter = decide_scrape_scope(job_counts)\n",
    "    \n",
    "    # Iterate each filter set\n",
    "    for day, worktype in filter_iter:\n",
    "        \n",
    "        # Skip filters already used--------------------------------------------------------\n",
    "        if [day, worktype] in used_filter:\n",
    "            continue\n",
    "        # Skip filters already used--------------------------------------------------------\n",
    "        \n",
    "        print(f\"\\n#####   |Day: {day}, Worktype: {worktype if worktype else 'All'}|   #####     \", end='')\n",
    "        # Set the path of csv file         \n",
    "        string = f\"{state.lower()} {day}d {worktype.lower().replace('-', '')}\".strip().replace(' ', '_')\n",
    "        csv_file = f'{string}.csv'\n",
    "        csv_path = f'{dataset_folder}/{state_folder}/{csv_file}'\n",
    "        \n",
    "        # Iterate each page for this filter to scrape jobs\n",
    "        total_job = current_scraped = empty_page = 0        \n",
    "        while total_job < 1000:\n",
    "            # Navigate to different api pages and scraped job listings by adjusting job_num, maximum 1000\n",
    "            job_filter.update(day=day, worktype=worktype, job_num=total_job)\n",
    "            url = job_filter.create_url()\n",
    "            result = scrape_this_page(url)\n",
    "            \n",
    "            if result != (0, 0, None):\n",
    "                empty_page = 0\n",
    "                job_scraped, jobs_in_page, df_page = result\n",
    "            else:\n",
    "                if empty_page > 3:\n",
    "                    break\n",
    "                empty_page += 1\n",
    "                total_job += 5\n",
    "                print(f'{current_scraped}/{total_job}+10 ', end='')\n",
    "                continue\n",
    "                \n",
    "            current_scraped += job_scraped\n",
    "            total_job += jobs_in_page\n",
    "            # Create a new csv file if it doesn't exist, else add new data to the file\n",
    "            if not os.path.exists(csv_path):               \n",
    "                df_page.to_csv(f'{csv_path}', index=False)           \n",
    "            else:\n",
    "                df_page.to_csv(f'{csv_path}', mode='a', header=False, index=False)           \n",
    "            \n",
    "            print(f'{current_scraped}/{total_job} ', end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ecb844f7-9fd2-45c8-b1a1-4a21dcc52bca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "            --------------------------------------------------------------------------------------------------------\n",
      "                                              *****  Washington State  ***** \n",
      "\n",
      "                      Any time (8,979)               On-site (4,304)            Internship (37)                   \n",
      "                    ✓ Past month (5,952)             Hybrid (899)               Entry level (690)                 \n",
      "                      Past week (1,649)              Remote (816)               Associate (148)                   \n",
      "                      Past 24 hours (290)                                       Mid-Senior level (3,412)          \n",
      "                                                                                Director (38)                     \n",
      "           --------------------------------------------------------------------------------------------------------\n",
      "\n",
      "#####   |Day: 7, Worktype: On-site|   #####     ......................................................54/60 ........62/70 ..........72/80 .......79/90 ..........89/100 ..........99/110 ..........109/120   *** EMPTY PAGE ***109/120+5 ..........119/135 .........128/145 ........136/154 .........145/163 ..........155/173 ..........165/183 ..........175/193 .......182/203 ..........192/213 .......199/223 ........207/233 ..........217/243 ..........227/253   *** EMPTY PAGE ***227/253+5   *** EMPTY PAGE ***227/258+5 ..........237/273 ..........247/283 .........256/293 ....260/303 .........269/313 .........278/323 ..........288/333 .........297/343 ........305/353 .........314/363 .........323/373 ......329/380 .......336/390 ..........346/400 .......353/409 .........362/419 .......369/429 ........377/439 ........385/449 .........394/459   *** EMPTY PAGE ***394/459+5 .........403/474 .........412/484 .........421/494 ..........431/504 ........439/514 .........448/524 ........456/534 .......463/544 ..........473/554   *** EMPTY PAGE ***473/554+5   *** EMPTY PAGE ***473/559+5 .........482/574 .........491/584 ..........501/594 ..........511/604 ..........521/614 .........530/624 ..........540/634 .........549/643 ..........559/653 .........568/663 .........577/673 .........586/683 .........595/693 ..........605/703 .........614/713 ........622/723 ........630/733 .........639/743 ..........649/753 .........658/763 ........666/773 .........675/783 .......682/793 .........691/803 .........700/813 ..........710/823 ........718/833 .........727/843 .........736/853 .........745/863 .........754/873 ........762/883 ..........772/893 ..........782/903 .......789/913 ........797/923 .........806/933   *** EMPTY PAGE ***806/933+5 ..........816/948 .......823/958 .........832/968 ..........842/978 ....846/983 ........854/993 ......860/1000 \n",
      "#####   |Day: 7, Worktype: Remote|   #####     ...................................................51/60 .........60/70 ........68/80 ........76/90 ..........86/100 .........95/110   *** EMPTY PAGE ***95/110+5 .........104/125 ........112/135   *** EMPTY PAGE ***112/135+5 ........120/150 .......127/160 .........136/170 ........144/180 ........152/190 ........160/200 ........168/210 ........176/220 ........184/230 ..186/233   *** EMPTY PAGE ***186/233+5   *** EMPTY PAGE ***186/238+5   *** EMPTY PAGE ***186/243+5   *** EMPTY PAGE ***186/248+5   *** EMPTY PAGE ***\n",
      "#####   |Day: 7, Worktype: Hybrid|   #####     .........................................................57/60 ..........67/70 .........76/80 .........85/90 ........93/100 .........102/110 .......109/120 ..........119/130 ..........129/140 ..........139/150 .........148/160 ..........158/170   *** EMPTY PAGE ***158/170+5 .......165/185 ..........175/195 ........183/205   *** EMPTY PAGE ***183/205+5 ..........193/220 ........201/229 .........210/239 ........218/248 ..........228/258 .........237/268 .........246/278 ..........256/288 ........264/298 ..........274/308 .........283/318 ........291/326   *** EMPTY PAGE ***291/326+5 .292/332   *** EMPTY PAGE ***292/332+5   *** EMPTY PAGE ***292/337+5   *** EMPTY PAGE ***292/342+5   *** EMPTY PAGE ***292/347+5   *** EMPTY PAGE ***\n",
      "#####   |Day: 21, Worktype: On-site|   #####     ....................................................52/60 .........61/70 ..........71/80 .........80/90 .........89/100 ..........99/110 ........107/120 ........115/130 .........124/140   *** EMPTY PAGE ***124/140+5 .........133/155   *** EMPTY PAGE ***133/155+5 ........141/170 ..........151/180 .........160/190 .........169/200 ..........179/210 .......186/220 .........195/230 .........204/240 .........213/250 ........221/260 .........230/270 .........239/280 .........248/290 .......255/300 ..........265/310 ........273/320 ........281/330 ..........291/340 .........300/350 ........308/359 ..........318/369 .........327/379 ..........337/389 .........346/399 ..........356/409 ..........366/419 ........374/429 .......381/439 ........389/449 ..........399/459 .........408/469 ........416/479   *** EMPTY PAGE ***416/479+5 .........425/494 ..........435/504 ........443/513 .........452/523 ..........462/533 ..........472/543 ..........482/553 ........490/563 .........499/573 .........508/582 ..........518/592 ..........528/602 .........537/612 .........546/622 ..........556/632 ..........566/642 .........575/652 ........583/662 ........591/672 .........600/682   *** EMPTY PAGE ***600/682+5 .........609/697 ........617/707 ..........627/717   *** EMPTY PAGE ***627/717+5 .........636/732 .........645/742 .........654/752 .........663/762 ..........673/772   *** EMPTY PAGE ***673/772+5 .........682/787 ..........692/797 ..........702/807 .........711/817 .........720/827 ........728/837 .........737/847 ..........747/857 .........756/867 ........764/877 .......771/887 ......777/897 .........786/907 ..........796/917 .........805/927   *** EMPTY PAGE ***805/927+5 .........814/942 ........822/952 ..........832/962 ........840/972   *** EMPTY PAGE ***840/972+5   *** EMPTY PAGE ***840/977+5 .........849/992 .......856/1000 \n",
      "#####   |Day: 21, Worktype: Remote|   #####     ...................................................51/60 ..........61/70 ........69/80 .........78/90 .........87/100 ..........97/110   *** EMPTY PAGE ***97/110+5 .........106/125 .......113/135 ..........123/145 ..........133/155 .........142/165 .........151/175 ........159/185 .......166/195   *** EMPTY PAGE ***166/195+5 .......173/210 ..........183/220 .......190/230 .........199/240 ..........209/250 .........218/260 .........227/270 ......233/280 .........242/290 ..........252/300 .........261/310 .........270/320 ......276/330 .........285/340 ..........295/350 ........303/360 ..........313/370 .........322/380 .........331/390 ........339/400 ........347/410 .........356/420 .........365/430 ..........375/440 ........383/450 ........391/460 .........400/470 ..........410/480 ..........420/490 .......427/500 .........436/510 .........445/520   *** EMPTY PAGE ***445/520+5 ..........455/535 ..........465/545 .........474/555 ........482/565 .........491/575 ..........501/585 .......508/595 ..........518/605 ........526/615 ........534/625   *** EMPTY PAGE ***534/625+5 ..........544/640 ..........554/650 .........563/660 .......570/670 ..........580/680 ......586/686 586/688   *** EMPTY PAGE ***586/688+5   *** EMPTY PAGE ***586/693+5   *** EMPTY PAGE ***586/698+5   *** EMPTY PAGE ***586/703+5   *** EMPTY PAGE ***\n",
      "#####   |Day: 21, Worktype: Hybrid|   #####     ....................................................52/60 .........61/70 ..........71/80 ........79/90 .......86/99 ..........96/109 .........105/119 ..........115/129 ..........125/139 ..........135/149 .........144/159 ..........154/169 ........162/179 ..........172/189 .........181/199 .........190/209 .......197/219 ..........207/229 .........216/239 ........224/249 ..........234/259 ..........244/269 ..........254/279 ..........264/289 ..........274/299 ..........284/309 ..........294/319 ..........304/329 ........312/338 ..........322/348 .......329/356 .........338/365 ..........348/375 .........357/385 ..........367/395 ..........377/405 .........386/414 ..........396/424 ..........406/434 ..........416/444 ..........426/454 ..........436/464 ..........446/474 .........455/484 .........464/494 ..........474/504 .........483/513 ..........493/523 ..........503/533 ..........513/543 ..........523/553 ..........533/563 ..........543/573 .........552/582 ..........562/592 ........570/602 .........579/612 ..........589/622 .........598/632 ........606/642 ..........616/652 ..........626/662 ........634/672 ..........644/682 .........653/692 ..........663/702 ..........673/712 .........682/722 .......689/732 ..........699/742 .......706/749 ..........716/759   *** EMPTY PAGE ***716/759+5   *** EMPTY PAGE ***716/764+5   *** EMPTY PAGE ***716/769+5   *** EMPTY PAGE ***716/774+5   *** EMPTY PAGE ***\n",
      "#####   |Day: 30, Worktype: On-site|   #####     .......................................................55/60 .........64/70 ........72/80 ..........82/90 ..........92/100 .........101/110 ........109/120 .........118/130 ..........128/140 ......134/150 .........143/160 .........152/170 ..........162/180 ..........172/190 .........181/200 .........190/210 .........199/220 .........208/230 ..........218/240 ......224/250 ..........234/260 .........243/270 ..........253/280 ........261/290 ..........271/300 ..........281/310 ..........291/320 ..........301/330 ........309/340 ..........319/350   *** EMPTY PAGE ***319/350+5 ........327/364 ..........337/374 .........346/384 .........355/394 ..........365/404 .........374/414 ..........384/424 .......391/434 .........400/444 .........409/454 ........417/464 .........426/474 ..........436/484 .........445/494 ........453/504 .........462/514 ..........472/524 ..........482/534 .........491/544 .........500/554   *** EMPTY PAGE ***500/554+5 ..........510/569 .........519/579 ........527/589 ........535/599 ........543/609 ..........553/619   *** EMPTY PAGE ***553/619+5 ..........563/634 ........571/644 ..........581/654 .........590/663   *** EMPTY PAGE ***590/663+5 ..........600/678 ..........610/688 ..........620/698 ..........630/708 .........639/718 ........647/728 ..........657/738 .........666/747 .........675/757 .........684/767 .......691/777 .........700/787 .........709/797 .........718/807 .........727/817   *** EMPTY PAGE ***727/817+5 ..........737/832 ..........747/842 ..........757/852 .........766/862 ..........776/872 .........785/882 ........793/892 ..........803/902 ..........813/912 ..........823/922 ..........833/932 ..........843/942 ........851/952 ..........861/962 .........870/972 ..........880/982 ..........890/992 ........898/1000 \n",
      "#####   |Day: 30, Worktype: Remote|   #####     ..........................................................58/60 ..........68/70 ..........78/80 ..........88/90 ..........98/100 .........107/110 ..........117/120 ..........127/130 ........135/140 ........143/150 ........151/160 .........160/170 .........169/180 .........178/190 ..........188/200 ..........198/210 ..........208/220 .........217/230 ..........227/240   *** EMPTY PAGE ***227/240+5 ..........237/255 .........246/265 .........255/275   *** EMPTY PAGE ***255/275+5 ..........265/290 .........274/300 ..........284/310 ..........294/320 .........303/330 ........311/340 .........320/350 .........329/360 ......335/370 ......341/380 .........350/390 .......357/400 ..........367/410 .........376/420 .......383/430 ..........393/440 .........402/450 ........410/460 ........418/470 .........427/480   *** EMPTY PAGE ***427/480+5 ......433/495 ........441/505 ......447/515 .......454/525   *** EMPTY PAGE ***454/525+5 ........462/540 ........470/550 ........478/560 ........486/570   *** EMPTY PAGE ***486/570+5 .........495/585 .........504/595 ........512/605 ..........522/615 .......529/625 .........538/635   *** EMPTY PAGE ***538/635+5 ........546/650 ........554/660   *** EMPTY PAGE ***554/660+5 .........563/675 ..........573/685 .........582/695 .........591/705 ..........601/715 ..........611/725 .........620/735 .........629/745 .........638/755 .........647/765 ..........657/775 ........665/785 .........674/795 ........682/805 .......689/815 .690/816   *** EMPTY PAGE ***690/816+5   *** EMPTY PAGE ***690/821+5   *** EMPTY PAGE ***690/826+5   *** EMPTY PAGE ***690/831+5   *** EMPTY PAGE ***\n",
      "#####   |Day: 30, Worktype: Hybrid|   #####     .................................................49/60 ..........59/70 .........68/80 .......75/90 ........83/100 .........92/110 ..........102/120 .......109/130   *** EMPTY PAGE ***109/130+5 .......116/145   *** EMPTY PAGE ***116/145+5 ..........126/160 .........135/170 .......142/180 ..........152/190 ..........162/200 .........171/210 ........179/220 ..........189/230 ........197/240 .........206/250 ......212/260 .........221/270 ..........231/280 .......238/290   *** EMPTY PAGE ***238/290+5 .........247/305 ..........257/315 .......264/325 ..........274/335 ........282/345 ..........292/355 .......299/365 .........308/375 .........317/385 ......323/395 .......330/405 ........338/415 .......345/425 ......351/435 ..........361/445 ..........371/455 .....376/465 ........384/475   *** EMPTY PAGE ***384/475+5 .......391/490 .......398/499 ..........408/509 .........417/519 ..........427/529 .......434/539 ..........444/549 .......451/559 .......458/569 ........466/579 .......473/589 ..........483/599 .......490/609 .......497/619 .........506/629 ..........516/639   *** EMPTY PAGE ***516/639+5 .........525/654 .........534/664 .......541/674 ..........551/684 ..........561/694 ........569/704 ..........579/714 .........588/724 ..........598/734 ........606/744 ........614/754 .........623/764   *** EMPTY PAGE ***623/764+5 ..........633/779 .........642/789 ........650/798 ........658/808 .........667/818 ..........677/828 ..........687/838 .........696/848 .........705/858 ........713/868 .......720/878 .........729/888 .730/889 ...733/892 .........742/902   *** EMPTY PAGE ***742/902+5   *** EMPTY PAGE ***742/907+5   *** EMPTY PAGE ***742/912+5   *** EMPTY PAGE ***742/917+5   *** EMPTY PAGE ***\n",
      "            ---------------------------------------------------------------------------------------------------\n",
      "                                              *****  Alabama State  ***** \n",
      "\n",
      "                      Any time (2,041)              On-site (739)            Internship (16)                 \n",
      "                    ✓ Past month (1,154)            Hybrid (285)             Entry level (213)               \n",
      "                      Past week (445)               Remote (119)             Associate (24)                  \n",
      "                      Past 24 hours (98)                                     Mid-Senior level (539)          \n",
      "                                                                             Director (1)                    \n",
      "           ---------------------------------------------------------------------------------------------------\n",
      "\n",
      "#####   |Day: 30, Worktype: All|   #####     ...................................................51/60 ........59/70 ........67/80   *** EMPTY PAGE ***67/80+5 ........75/95 ..........85/105 ..........95/115 .......102/125 ........110/135 ..........120/145 ........128/155 ........136/165 ......142/175 ........150/184 .........159/194 ........167/204 .........176/214 .......183/224 ........191/234   *** EMPTY PAGE ***191/234+5 .........200/249 ......206/259 ..........216/269 ......222/279 .........231/289 .........240/299   *** EMPTY PAGE ***240/299+5 .......247/314   *** EMPTY PAGE ***247/314+5 .......254/328 .........263/337 .........272/347 ........280/357 .........289/367 ..........299/377 ......305/386 ........313/396 .........322/405 ........330/415 .........339/425 .........348/435 ......354/444   *** EMPTY PAGE ***354/444+5 ......360/459 .......367/469   *** EMPTY PAGE ***367/469+5   *** EMPTY PAGE ***367/474+5 ..........377/489 .........386/498 ........394/508 ..........404/518 ........412/528 .........421/538 .........430/548 .........439/557 ......445/566 .......452/574 ........460/583 ..........470/593 .........479/603 ......485/613 .........494/622 .........503/632 ..........513/642 ..........523/652 .......530/661   *** EMPTY PAGE ***530/661+5 .........539/676 .....544/686 .........553/696 ........561/706 ........569/716 .........578/726   *** EMPTY PAGE ***578/726+5 ..........588/741 ..........598/751 .........607/761 ..........617/771 ........625/780 ........633/790 ..........643/800 .........652/810 .......659/820 ........667/830 ..........677/840 .........686/850 ........694/860 ........702/870 ....706/880   *** EMPTY PAGE ***706/880+5 .........715/895 ..........725/905 ........733/915 ......739/925 .........748/935 ..........758/945 ..........768/955 .........777/965 ........785/975 ..........795/985 ..........805/995 ....809/1000 \n",
      "            --------------------------------------------------------------------------------------------------------\n",
      "                                                *****  Arizona State  ***** \n",
      "\n",
      "                      Any time (4,087)               On-site (1,328)            Internship (20)                   \n",
      "                    ✓ Past month (2,339)             Hybrid (598)               Entry level (474)                 \n",
      "                      Past week (887)                Remote (414)               Associate (68)                    \n",
      "                      Past 24 hours (161)                                       Mid-Senior level (1,243)          \n",
      "                                                                                Director (4)                      \n",
      "           --------------------------------------------------------------------------------------------------------\n",
      "\n",
      "#####   |Day: 7, Worktype: On-site|   #####     .......................................................55/60 ........63/70 ........71/80 ........79/90 ..........89/100 ..........99/110 .......106/120 .........115/130 .........124/140 ..........134/150 .........143/160 ..........153/170 ........161/180 .........170/190 ..........180/200 .........189/210 ..........199/220 ..........209/230 ........217/240 .........226/250 ........234/260 ........242/270 ..........252/280 .........261/290 ..........271/300 .........280/310 ..........290/320 ..........300/330 ..........310/340 .........319/350 .........328/360 ..........338/370 ..........348/380 ..........358/390 ..........368/400 .........377/410 ........385/420 ..........395/430 .........404/440 .........413/450 ..........423/460 ..........433/470 ..........443/480 ....447/484 .......454/491   *** EMPTY PAGE ***454/491+5   *** EMPTY PAGE ***454/496+5   *** EMPTY PAGE ***454/501+5   *** EMPTY PAGE ***454/506+5   *** EMPTY PAGE ***\n",
      "#####   |Day: 7, Worktype: Remote|   #####     ...........................................................59/60 ..........69/70 ..........79/80 ........87/90 ..........97/100 ..........107/110 .........116/120 .........125/130 ..........135/140 .........144/150 .......151/159   *** EMPTY PAGE ***151/159+5   *** EMPTY PAGE ***151/164+5   *** EMPTY PAGE ***151/169+5   *** EMPTY PAGE ***151/174+5   *** EMPTY PAGE ***\n",
      "#####   |Day: 7, Worktype: Hybrid|   #####     ...........................................................59/60 ..........69/70 ..........79/80 ..........89/90 ..........99/100 ..........109/110 ..........119/120 ..........129/130 .........138/139 ..........148/149 .........157/159 ..........167/169 ..........177/179 ..........187/189 ..........197/199 ..........207/209 ..........217/219 ..........227/229 ..........237/239 ..........247/249 ........255/257 ...258/260 ..260/262   *** EMPTY PAGE ***260/262+5   *** EMPTY PAGE ***260/267+5   *** EMPTY PAGE ***260/272+5   *** EMPTY PAGE ***260/277+5   *** EMPTY PAGE ***\n",
      "#####   |Day: 21, Worktype: On-site|   #####     ............................................................60/60 ..........70/70 ..........80/80 ..........90/90 ..........100/100 ..........110/110 ..........120/120 ..........130/130 ..........140/140 ..........150/150 ..........160/160 ..........170/170 ..........180/180 .........189/190 ..........199/200 ........207/210 ..........217/220 ..........227/230 ..........237/240 ..........247/250 ..........257/260 ..........267/270 ..........277/280 ..........287/290 ..........297/300 ..........307/310 ..........317/320 ..........327/330 ..........337/340 ..........347/350 .........356/360 ..........366/370 .........375/380 ..........385/390 ..........395/400 ..........405/410 ..........415/420 ..........425/430 ..........435/440 ..........445/450 ..........455/460 .........464/470 ..........474/480 ..........484/490 ..........494/500 ..........504/510 ..........514/520 ..........524/530 ..........534/540 ..........544/550 ..........554/560 ..........564/570 ..........574/580 ..........584/590 ..........594/600 ..........604/610 ..........614/620 ..........624/630 ..........634/640 ..........644/650 ..........654/660 ..........664/670 ..........674/680 ..........684/690 ..........694/700 ..........704/710 ..........714/720 ..........724/730 ..........734/740 .........743/750 ..........753/760 ..........763/770 ..........773/780 ..........783/790 ..........793/800 ..........803/810 ..........813/820 ..........823/830 ..........833/840 ..........843/850 ..........853/860 ..........863/870 ..........873/880   *** EMPTY PAGE ***873/880+5 ..........883/895 .........892/904 ..........902/914 ..........912/924 ..........922/934 ..........932/944 ..........942/954 ..........952/964 .........961/974 ..........971/984 ..........981/994 ......987/1000 \n",
      "#####   |Day: 21, Worktype: Remote|   #####     ............................................................60/60 ..........70/70 ..........80/80 ..........90/90 ..........100/100 ..........110/110 ..........120/120 ..........130/130 ..........140/140 ..........150/150 ..........160/160 ..........170/170 ..........180/180 ..........190/190 ..........200/200 ..........210/210 ..........220/220 ..........230/230 ..........240/240 ..........250/250 ..........260/260 ..........270/270 ........278/280 ..........288/290 ..........298/300 ..........308/310 ..310/312 ...313/315   *** EMPTY PAGE ***313/315+5   *** EMPTY PAGE ***313/320+5   *** EMPTY PAGE ***313/325+5   *** EMPTY PAGE ***313/330+5   *** EMPTY PAGE ***\n",
      "#####   |Day: 21, Worktype: Hybrid|   #####     ...........................................................59/59 ..........69/69 ..........79/79 ..........89/89 ..........99/99 ..........109/109 ..........119/119 ..........129/129 ..........139/139 ..........149/149 ..........159/159 ..........169/169 ..........179/179 ..........189/189 ..........199/199 ..........209/209 ..........219/219 ..........229/229 ..........239/239 ..........249/249 ..........259/259 ..........269/269 ..........279/279 ..........289/289 ..........299/299 ..........309/309 ..........319/319 ..........329/329 ..........339/339 ..........349/349 ..........359/359 ..........369/369 ..........379/379 ..........389/389 ..........399/399 .........408/409 ..........418/419 ..........428/429 ..........438/439 ..........448/449 ..........458/459 ..........468/469 ..........478/479 ..........488/489 ..........498/499 ..........508/509 ..........518/519 ..........528/529 .......535/536   *** EMPTY PAGE ***535/536+5   *** EMPTY PAGE ***535/541+5   *** EMPTY PAGE ***535/546+5   *** EMPTY PAGE ***535/551+5   *** EMPTY PAGE ***\n",
      "#####   |Day: 30, Worktype: On-site|   #####     ............................................................60/60 ..........70/70 ..........80/80 ..........90/90 ..........100/100 ..........110/110 ..........120/120 ..........130/130 ..........140/140 ..........150/150 ..........160/160 ..........170/170 ..........180/180 ..........190/190 ..........200/200 ..........210/210 ..........220/220 ..........230/230 ..........240/240 ..........250/250 ..........260/260 ..........270/270 ..........280/280 ..........290/290 ..........300/300 ..........310/310 ..........320/320 ..........330/330 ..........340/340 ..........350/350 .........359/360 ..........369/370 ..........379/380 .........388/390 ..........398/400 ..........408/410 .........417/420 .........426/430 ..........436/440 ..........446/450 ..........456/460 ..........466/470 .........475/480 ..........485/490 .........494/500 ..........504/510 .........513/519 ..........523/529 .........532/539 ..........542/549 .........551/559 ..........561/569 ..........571/579 ..........581/589 ..........591/599 .........600/609 ..........610/619 .........619/629 ..........629/639 ..........639/649 ..........649/659 ..........659/669 ..........669/679 ..........679/689 ..........689/699 ..........699/709 ..........709/719 ..........719/729 ..........729/739 .........738/749 ..........748/759 ..........758/769 ..........768/779 ........776/789 ..........786/799 .........795/809 ..........805/819 .........814/829 .........823/839 ..........833/849 ..........843/859 ..........853/869 .........862/879 ..........872/889 ..........882/899 ..........892/909 ..........902/919 ..........912/929 ..........922/939 ..........932/949 ..........942/959 ..........952/969 ..........962/979 ..........972/989 ..........982/999 .983/1000 \n",
      "#####   |Day: 30, Worktype: Remote|   #####     ..........................................................58/60 ..........68/70 ........76/80 ..........86/90 .........95/100 .........104/110 ..........114/120 ..........124/130 ..........134/140 .........143/150 ..........153/160 ..........163/170 .........172/180 ..........182/190 ..........192/200 .........201/210 ..........211/220 ..........221/230 ..........231/240 ..........241/250 ..........251/260 ..........261/270 ..........271/280 ..........281/290 ..........291/300 ..........301/310 .........310/320 ..........320/330 ..........330/340 ........338/350 ........346/360   *** EMPTY PAGE ***346/360+5 ..........356/375 .........365/385 .........374/395 .........383/405 ...386/409 .387/410   *** EMPTY PAGE ***387/410+5   *** EMPTY PAGE ***387/415+5   *** EMPTY PAGE ***387/420+5   *** EMPTY PAGE ***387/425+5   *** EMPTY PAGE ***\n",
      "#####   |Day: 30, Worktype: Hybrid|   #####     .........................................................57/60 ........65/69 ..........75/79 ..........85/89 ..........95/99 .........104/109 ..........114/119 ..........124/129 ..........134/139 ..........144/149 ..........154/159 ..........164/169 ..........174/179 ..........184/189 ..........194/199 ..........204/209 ..........214/219 ........222/228 ..........232/238 ..........242/248 .........251/258 ..........261/268 ..........271/278 ..........281/288 .........290/297 .........299/307 ..........309/317 ..........319/327 .........328/337 ..........338/347 .........347/357 ..........357/367   *** EMPTY PAGE ***357/367+5 ........365/382 ..........375/392 ........383/402 ..........393/412 ..........403/422 ..........413/432 .......420/442 ........428/451 ........436/460 ..........446/470 ..........456/480 .........465/490 .........474/500 ..........484/510 ..........494/520 .........503/530 ........511/540 .........520/550 ..........530/560 ..........540/570 ..........550/580 ..........560/590 ..........570/600   *** EMPTY PAGE ***570/600+5   *** EMPTY PAGE ***570/605+5   *** EMPTY PAGE ***570/610+5   *** EMPTY PAGE ***570/615+5   *** EMPTY PAGE ***\n",
      "            ---------------------------------------------------------------------------------------------------\n",
      "                                             *****  Arkansas State  ***** \n",
      "\n",
      "                      Any time (873)                On-site (378)            Internship (6)                  \n",
      "                    ✓ Past month (554)              Hybrid (93)              Entry level (104)               \n",
      "                      Past week (159)               Remote (83)              Associate (11)                  \n",
      "                      Past 24 hours (59)                                     Mid-Senior level (307)          \n",
      "           ---------------------------------------------------------------------------------------------------\n",
      "\n",
      "#####   |Day: 30, Worktype: All|   #####     ..........................................................58/60 .........67/70 ..........77/80 ..........87/90 .........96/100 .........105/110 ..........115/120 ..........125/130 ..........135/140 ........143/150   *** EMPTY PAGE ***143/150+5 ..........153/165 ........161/175 ..........171/185 ..........181/195 .........190/205 ..........200/215 ..........210/225 .........219/235 .........228/245 ..........238/255 ..........248/265 ..........258/275 .......265/285 ..........275/295 ........283/305 ..........293/315 ..........303/325 .........312/335 ..........322/345 ........330/355 .........339/365 ..........349/375 ..........359/385 ..........369/395 .........378/405 ..........388/415 .........397/425   *** EMPTY PAGE ***397/425+5 ..........407/440 .........416/450 ..........426/460 ........434/470 ..........444/480 ..........454/490 .........463/500 ..........473/510 ..........483/520 .........492/530 ........500/540 ..........510/550 .......517/557 ....521/561 ..523/563   *** EMPTY PAGE ***523/563+5   *** EMPTY PAGE ***523/568+5   *** EMPTY PAGE ***523/573+5   *** EMPTY PAGE ***523/578+5   *** EMPTY PAGE ***\n",
      "            --------------------------------------------------------------------------------------------------------\n",
      "                                               *****  Colorado State  ***** \n",
      "\n",
      "                      Any time (4,594)               On-site (1,769)            Internship (17)                   \n",
      "                    ✓ Past month (2,702)             Hybrid (615)               Entry level (445)                 \n",
      "                      Past week (1,080)              Remote (317)               Associate (104)                   \n",
      "                      Past 24 hours (209)                                       Mid-Senior level (1,535)          \n",
      "                                                                                Director (10)                     \n",
      "           --------------------------------------------------------------------------------------------------------\n",
      "\n",
      "#####   |Day: 7, Worktype: On-site|   #####     .......................................................55/59 ..........65/69 ..........75/79 .........84/89 ..........94/99 ..........104/109 .........113/119 .........122/128 .........131/138 ........139/148 ........147/157 .........156/167 ..........166/177 ..........176/187 ..........186/197 ..........196/207 .......203/217 .........212/227   *** EMPTY PAGE ***212/227+5 ..........222/242 ........230/251 ..........240/261 ..........250/271 .........259/281 ..........269/291 ..........279/301 ..........289/311 ..........299/321 .........308/331 ..........318/341 ..........328/351 ..........338/361 .........347/370 .........356/379 ..........366/389 ..........376/399 ..........386/409 ..........396/419 ..........406/429 ..........416/439 ..........426/449 ..........436/459 ..........446/469 ..........456/479 .......463/488 ..........473/498 ........481/508 ..........491/518 .........500/528 ..........510/538 ..........520/548 ..........530/558 ........538/568 ..........548/578 .........557/587 .........566/596 ..........576/606 ..........586/616 ..........596/626 ..........606/636 ..........616/646 .......623/655 .........632/664 ..........642/674 ..........652/684 ..........662/694 ..........672/704   *** EMPTY PAGE ***672/704+5   *** EMPTY PAGE ***672/709+5   *** EMPTY PAGE ***672/714+5   *** EMPTY PAGE ***672/719+5   *** EMPTY PAGE ***\n",
      "#####   |Day: 7, Worktype: Remote|   #####     .....................................................53/60 ........61/70 ..........71/80 ..........81/90 ..........91/100 ..........101/110 ..........111/120 ..........121/130 ....125/134 .....130/139 .131/140   *** EMPTY PAGE ***131/140+5   *** EMPTY PAGE ***131/145+5   *** EMPTY PAGE ***131/150+5   *** EMPTY PAGE ***131/155+5   *** EMPTY PAGE ***\n",
      "#####   |Day: 7, Worktype: Hybrid|   #####     ............................................................60/60 ..........70/70 ..........80/80 .........89/90 ..........99/100 .........108/109 ..........118/119 ..........128/129 .........137/139 ..........147/149 .........156/159 .........165/168 ..........175/178 .........184/187 ..........194/197 ..........204/207 ..........214/217 ..........224/227 .........233/236 ..........243/246 ...246/249 ..248/252   *** EMPTY PAGE ***248/252+5   *** EMPTY PAGE ***248/257+5   *** EMPTY PAGE ***248/262+5   *** EMPTY PAGE ***248/267+5   *** EMPTY PAGE ***\n",
      "#####   |Day: 21, Worktype: On-site|   #####     ..........................................................58/60 .........67/70 ..........77/80 ..........87/90 ..........97/100 ..........107/110 ..........117/120 ..........127/130 ..........137/140 ..........147/150 ..........157/160 ..........167/170 ..........177/180 ..........187/190 .........196/200 ..........206/210 ..........216/220 ..........226/230 ........234/240 ..........244/250 ..........254/260 ..........264/270 .........273/280 .........282/290 ..........292/300 ..........302/310 .........311/320 ..........321/330 .........330/339 .........339/348 ..........349/358 ..........359/368 ..........369/378 ..........379/388 ..........389/398 ..........399/408 ..........409/418 ..........419/428 .........428/438 ..........438/448   *** EMPTY PAGE ***438/448+5 ..........448/463 .........457/473 ..........467/483 ..........477/493 ..........487/503 ..........497/513 .........506/523 ........514/533 ..........524/543 ..........534/553 ..........544/563 ..........554/573 .........563/583 ..........573/593 ..........583/603 ..........593/613 ..........603/623 .........612/633 .........621/643 ..........631/653 ..........641/663 ..........651/673 ..........661/683 ..........671/693 ..........681/703 ..........691/713 ..........701/723 .........710/733 .........719/742 ..........729/752 .........738/762 ..........748/772 ........756/782 .........765/792 ..........775/802 .........784/812 ..........794/822 ..........804/832 .........813/842 ..........823/852 ..........833/862 .........842/872 .........851/882 .........860/892 ..........870/902 .........879/912 .........888/922 ..........898/932 .......905/942 .........914/951 .........923/960 ..........933/970 ........941/980 ..........951/990   *** EMPTY PAGE ***951/990+5 ....955/1000 \n",
      "#####   |Day: 21, Worktype: Remote|   #####     .......................................................55/60 ..........65/70 ..........75/80 ........83/90 ........91/100 ........99/110 ..........109/120 ........117/130 .........126/140 ..........136/150 ........144/160 .........153/170 ..........163/180 ..........173/190 ..........183/200 ........191/210 .........200/220 ..........210/230 ..........220/240 .....225/246   *** EMPTY PAGE ***225/246+5   *** EMPTY PAGE ***225/251+5   *** EMPTY PAGE ***225/256+5   *** EMPTY PAGE ***225/261+5   *** EMPTY PAGE ***\n",
      "#####   |Day: 21, Worktype: Hybrid|   #####     .....................................................53/60 .........62/70 .......69/80 .........78/90 ..........88/100 ..........98/110 ..........108/120 .........117/130 ..........127/140   *** EMPTY PAGE ***127/140+5 ........135/155 ........143/165 ........151/175 ........159/185 .........168/195 ........176/205 .......183/215 .........192/225 .........201/235 ........209/245 .......216/255 .........225/265   *** EMPTY PAGE ***225/265+5 .......232/279 ......238/289 ..........248/299 .........257/308 .......264/317 ..........274/327 .......281/337 ..........291/347 ........299/357 .........308/367 ..........318/377 ........326/387 ..........336/397   *** EMPTY PAGE ***336/397+5 ........344/412   *** EMPTY PAGE ***344/412+5   *** EMPTY PAGE ***344/417+5 .......351/432 .........360/442 ........368/451 ......374/461 .........383/470   *** EMPTY PAGE ***383/470+5   *** EMPTY PAGE ***383/475+5   *** EMPTY PAGE ***383/480+5   *** EMPTY PAGE ***383/485+5   *** EMPTY PAGE ***\n",
      "#####   |Day: 30, Worktype: On-site|   #####     ...................................................51/59 .........60/69 .......67/78 .......74/87 .........83/97 ........91/107 ......97/117 .......104/127 .........113/137 .........122/147 ........130/157 .........139/167 .........148/177 .........157/187 ..........167/197 .........176/207 .....181/217 .........190/227 ........198/237 ........206/247 ..........216/257 .........225/267 ..........235/277 ........243/287 ........251/297 .........260/307 ........268/317 ........276/327 .......283/337 ........291/346 ........299/356 .........308/366 .......315/376   *** EMPTY PAGE ***315/376+5 ..........325/391 ..........335/401 .........344/411 ........352/421   *** EMPTY PAGE ***352/421+5 ......358/436 .......365/446 ........373/455   *** EMPTY PAGE ***373/455+5 ..........383/470 ........391/480 ..........401/490 ..........411/500 .........420/510 ......426/520 ..........436/530 .......443/540 ....447/550 .......454/560 .......461/570 .......468/580 ..........478/590 .........487/600 ........495/610 .......502/620 .........511/630 .......518/639 ........526/648 ........534/658 .........543/668 .........552/678 .........561/688 .........570/698 ........578/708   *** EMPTY PAGE ***578/708+5 .........587/723 ........595/733 ........603/743   *** EMPTY PAGE ***603/743+5 ........611/758 ..........621/768 ..........631/778 ..........641/788 ..........651/798 ..........661/808 ..........671/818 .........680/828 .........689/838 .........698/848   *** EMPTY PAGE ***698/848+5 ........706/863 ........714/873 ..........724/883 ........732/893 ..........742/903   *** EMPTY PAGE ***742/903+5 .......749/918 ......755/927 ..........765/937 .........774/947 ..........784/957 .......791/967 .........800/977   *** EMPTY PAGE ***800/977+5 .........809/992 .......816/1000 \n",
      "#####   |Day: 30, Worktype: Remote|   #####     ................................................48/60 ......54/70 ........62/80 ..........72/90 ........80/100 ..........90/110 ..........100/120 .....105/130 ..........115/140 .......122/150   *** EMPTY PAGE ***122/150+5 .........131/165   *** EMPTY PAGE ***131/165+5 .........140/180 .........149/190   *** EMPTY PAGE ***149/190+5 ..........159/205 .........168/215 ........176/225 ........184/235 ..........194/245 ........202/255 .......209/265 .........218/275 .......225/285   *** EMPTY PAGE ***225/285+5   *** EMPTY PAGE ***225/290+5 .......232/305 ........240/314   *** EMPTY PAGE ***240/314+5   *** EMPTY PAGE ***240/319+5   *** EMPTY PAGE ***240/324+5   *** EMPTY PAGE ***240/329+5   *** EMPTY PAGE ***\n",
      "#####   |Day: 30, Worktype: Hybrid|   #####       *** EMPTY PAGE ***0/0+5 ........8/15 .....13/25 .......20/35 .........29/45 ..........39/55 ..........49/65 .........58/75 .........67/85 ........75/95 .........84/105 .........93/115 ..........103/125   *** EMPTY PAGE ***103/125+5 ........111/140 ..........121/150 ..........131/160 .........140/169 .........149/178 ..........159/188 ..........169/198 ........177/208 ..........187/218 .........196/228 ..........206/238 ..........216/248 ..........226/258 ..........236/268 ..........246/278 .........255/288 ........263/298 ........271/308 .........280/318 ........288/328 ....292/337 ..........302/347 ..........312/357 ........320/366 .........329/376 ........337/386 .......344/394 ........352/403 ........360/413 .........369/423 ..........379/433 .........388/443 .........397/453 ..........407/463 ........415/473 .....420/483 .........429/493   *** EMPTY PAGE ***429/493+5 .........438/508 ........446/518 .........455/528 .........464/538 ........472/548   *** EMPTY PAGE ***472/548+5 .........481/562 .......488/571 ......494/581 .........503/591 .........512/601 ..514/603 .........523/613   *** EMPTY PAGE ***523/613+5   *** EMPTY PAGE ***523/618+5   *** EMPTY PAGE ***523/623+5   *** EMPTY PAGE ***523/628+5   *** EMPTY PAGE ***\n",
      "            ---------------------------------------------------------------------------------------------------\n",
      "                                            *****  Connecticut State  ***** \n",
      "\n",
      "                      Any time (2,302)              On-site (915)            Internship (15)                 \n",
      "                    ✓ Past month (1,342)            Hybrid (276)             Entry level (235)               \n",
      "                      Past week (660)               Remote (155)             Associate (60)                  \n",
      "                      Past 24 hours (59)                                     Mid-Senior level (763)          \n",
      "                                                                             Director (1)                    \n",
      "           ---------------------------------------------------------------------------------------------------\n",
      "\n",
      "#####   |Day: 30, Worktype: On-site|   #####     ......................................................54/60 "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_33624\\2521547902.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[0mjob_filter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mday\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mday\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mworktype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworktype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjob_num\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_job\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m             \u001b[0murl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjob_filter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_url\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscrape_this_page\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_33624\\1276216793.py\u001b[0m in \u001b[0;36mscrape_this_page\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m     15\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtry_\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m             \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m             \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "title = 'Data Analyst' \n",
    "states = get_states()\n",
    "\n",
    "# Get used filters ---------------------------------------------------\n",
    "used_filter = get_used_filter()\n",
    "# Get used filters ---------------------------------------------------\n",
    "\n",
    "# Create a dataset file if it doesn't exist\n",
    "dataset_folder = 'datasets'\n",
    "if not os.path.exists(dataset_folder):\n",
    "    os.makedirs(dataset_folder)\n",
    "    \n",
    "# Iterate each state\n",
    "for state in states:\n",
    "    \n",
    "    # Skip used states -----------------------------------------------\n",
    "    if state in used_filter:\n",
    "        continue\n",
    "    # Skip used states -----------------------------------------------\n",
    "    \n",
    "    # Create a State file if it doesn't exist\n",
    "    state_folder = state.lower().replace(' ', '_')\n",
    "    if not os.path.exists(f'{dataset_folder}/{state_folder}'):\n",
    "        os.makedirs(f'{dataset_folder}/{state_folder}')\n",
    "    \n",
    "    location = state\n",
    "    if state != 'District of Columbia':\n",
    "        location = state + ' State'\n",
    "    job_filter = SetFilter(title=title, location=location, day=30)\n",
    "    job_counts = job_filter.show_job_counts()\n",
    "    filter_iter = decide_scrape_scope(job_counts)\n",
    "    \n",
    "    # Iterate each filter set\n",
    "    for day, worktype in filter_iter:\n",
    "        \n",
    "        # Skip filters already used--------------------------------------------------------\n",
    "        if [day, worktype] in used_filter:\n",
    "            continue\n",
    "        # Skip filters already used--------------------------------------------------------\n",
    "        \n",
    "        print(f\"\\n#####   |Day: {day}, Worktype: {worktype if worktype else 'All'}|   #####     \", end='')\n",
    "        # Set the path of csv file         \n",
    "        string = f\"{state.lower()} {day}d {worktype.lower().replace('-', '')}\".strip().replace(' ', '_')\n",
    "        csv_file = f'{string}.csv'\n",
    "        csv_path = f'{dataset_folder}/{state_folder}/{csv_file}'\n",
    "        \n",
    "        # Iterate each page for this filter to scrape jobs\n",
    "        total_job = current_scraped = empty_page = 0        \n",
    "        while total_job < 1000:\n",
    "            # Navigate to different api pages and scraped job listings by adjusting job_num, maximum 1000\n",
    "            job_filter.update(day=day, worktype=worktype, job_num=total_job)\n",
    "            url = job_filter.create_url()\n",
    "            result = scrape_this_page(url)\n",
    "            \n",
    "            if result != (0, 0, None):\n",
    "                empty_page = 0\n",
    "                job_scraped, jobs_in_page, df_page = result\n",
    "            else:\n",
    "                if empty_page > 3:\n",
    "                    break\n",
    "                empty_page += 1\n",
    "                print(f'{current_scraped}/{total_job}+5 ', end='')\n",
    "                total_job += 5\n",
    "                continue\n",
    "                \n",
    "            current_scraped += job_scraped\n",
    "            total_job += jobs_in_page\n",
    "            # Create a new csv file if it doesn't exist, else add new data to the file\n",
    "            if not os.path.exists(csv_path):               \n",
    "                df_page.to_csv(f'{csv_path}', index=False)           \n",
    "            else:\n",
    "                df_page.to_csv(f'{csv_path}', mode='a', header=False, index=False)           \n",
    "            \n",
    "            print(f'{current_scraped}/{total_job} ', end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "06732b71-fd9b-4fed-ab75-fc2b596fc28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b971f7e1-d39d-45ee-9ad7-6d230cc25188",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a26b93-aeea-4669-ba51-6e9532375088",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
